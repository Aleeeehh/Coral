{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "inU1WWTTcHjg"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import sys\n",
        "print(\"Python path:\", sys.executable)\n",
        "print(\"Python version:\", sys.version)\n",
        "\n",
        "#Carica il modello preaddestrato più piccolo\n",
        "model = YOLO(\"yolo11l.pt\")\n",
        "\n",
        "# Esporta in formato TFLite quantizzato \n",
        "model.export(format='tflite', int8=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1j22semIVEhS"
      },
      "outputs": [],
      "source": [
        "# --- Funzione di preprocess immagine MODIFICATA per salvare l'immagine ---\n",
        "def preprocess_image(img_path, save_preprocessed=True):\n",
        "    # Carica immagine con PIL (RGB)\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # Immagine originale dimensioni 640x480\n",
        "    original_width, original_height = img.size\n",
        "    print(f\"Original image size: {original_width}x{original_height}\")\n",
        "\n",
        "    # Target size modello: 640x640\n",
        "    target_size = 640\n",
        "\n",
        "    # Crea canvas nero quadrato 640x640\n",
        "    new_img = Image.new('RGB', (target_size, target_size), (0, 0, 0))\n",
        "\n",
        "    # Calcola offset per centrare l'immagine originale sul canvas\n",
        "    offset_x = 0\n",
        "    offset_y = (target_size - original_height) // 2  # vertical padding\n",
        "\n",
        "    # Incolla immagine originale al centro (horizontal no padding)\n",
        "    new_img.paste(img, (offset_x, offset_y))\n",
        "\n",
        "    # Salva l'immagine preprocessata se richiesto\n",
        "    if save_preprocessed:\n",
        "        preprocessed_path = img_path.replace('.jpg', '_preprocessed.jpg').replace('.png', '_preprocessed.png')\n",
        "        new_img.save(preprocessed_path)\n",
        "        print(f\"Immagine preprocessata salvata come: {preprocessed_path}\")\n",
        "\n",
        "    # Converti in numpy array float32 e normalizza (se modello vuole 0-1)\n",
        "    input_array = np.array(new_img).astype(np.float32) / 255.0\n",
        "\n",
        "    # Aggiungi dimensione batch (1, 640, 640, 3)\n",
        "    input_array = np.expand_dims(input_array, axis=0)\n",
        "\n",
        "    return input_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image_320(img_path, save_preprocessed=True):\n",
        "    # Carica immagine con PIL (RGB) - già 320x320\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    \n",
        "    print(f\"Immagine originale: {img.size}\")\n",
        "    \n",
        "    # Salva se richiesto (opzionale, per debug)\n",
        "    if save_preprocessed:\n",
        "        preprocessed_path = img_path.replace('.jpg', '_preprocessed_320.jpg')\n",
        "        img.save(preprocessed_path)\n",
        "        print(f\"Immagine preprocessata salvata come: {preprocessed_path}\")\n",
        "    \n",
        "    # Solo normalizzazione e formato - NESSUN RESIZE\n",
        "    input_array = np.array(img).astype(np.float32) / 255.0\n",
        "    input_array = np.expand_dims(input_array, axis=0)  # (1, 320, 320, 3)\n",
        "    \n",
        "    return input_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Funzione di post-processing\n",
        "def detect_person(output_data, confidence_threshold=0.5, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Post-processing per rilevare persone nell'immagine\n",
        "    \"\"\"\n",
        "    # COCO class ID per 'person' è 0\n",
        "    PERSON_CLASS_ID = 0 #dovrebbe essere 0 di default\n",
        "    \n",
        "    # Estrai le predizioni (rimuovi dimensione batch)\n",
        "    predictions = output_data[0]  # Shape: (84, 8400)\n",
        "    \n",
        "    # Separa coordinate, confidence e classi\n",
        "    # Ogni colonna: [x, y, w, h, confidence, class1, class2, ..., class80]\n",
        "    boxes = predictions[:4, :].T  # (8400, 4) - coordinate relative\n",
        "    confidences = predictions[29, :]  # (8400,) - confidence scores\n",
        "    class_scores = predictions[5:, :].T  # (8400, 79) - class scores\n",
        "    \n",
        "    # Trova le detections per la classe 'person'\n",
        "    person_scores = class_scores[:, PERSON_CLASS_ID]  # (8400,)\n",
        "    \n",
        "    # Combina confidence generale con confidence specifica della classe\n",
        "    final_scores = confidences * person_scores\n",
        "    \n",
        "    # Filtra detections con score alto\n",
        "    high_score_indices = np.where(final_scores > confidence_threshold)[0]\n",
        "    \n",
        "    if len(high_score_indices) == 0:\n",
        "        print(\"❌ Nessuna persona rilevata nell'immagine\")\n",
        "        return []\n",
        "    \n",
        "    # Estrai bounding boxes e scores\n",
        "    detected_boxes = []\n",
        "    detected_scores = []\n",
        "    \n",
        "    for idx in high_score_indices:\n",
        "        # Converti coordinate relative in pixel\n",
        "        x_center, y_center, width, height = boxes[idx]\n",
        "        \n",
        "        # Converti da formato YOLO (center, width, height) a formato pixel (x1, y1, x2, y2)\n",
        "        x1 = int((x_center - width/2) * 640)\n",
        "        y1 = int((y_center - height/2) * 640)\n",
        "        x2 = int((x_center + width/2) * 640)\n",
        "        y2 = int((y_center + height/2) * 640)\n",
        "        \n",
        "        # Clamp ai bordi dell'immagine\n",
        "        x1 = max(0, min(x1, 640))\n",
        "        y1 = max(0, min(y1, 640))\n",
        "        x2 = max(0, min(x2, 640))\n",
        "        y2 = max(0, min(y2, 640))\n",
        "        \n",
        "        detected_boxes.append([x1, y1, x2, y2])\n",
        "        detected_scores.append(final_scores[idx])\n",
        "    \n",
        "    # Non-maximum suppression per rimuovere duplicati\n",
        "    if len(detected_boxes) > 1:\n",
        "        # Implementazione semplificata di NMS\n",
        "        keep_indices = []\n",
        "        for i in range(len(detected_boxes)):\n",
        "            keep = True\n",
        "            for j in range(len(detected_boxes)):\n",
        "                if i != j and detected_scores[j] > detected_scores[i]:\n",
        "                    # Calcola IoU\n",
        "                    box1 = detected_boxes[i]\n",
        "                    box2 = detected_boxes[j]\n",
        "                    \n",
        "                    # Calcola area di intersezione\n",
        "                    x1 = max(box1[0], box2[0])\n",
        "                    y1 = max(box1[1], box2[1])\n",
        "                    x2 = min(box1[2], box2[2])\n",
        "                    y2 = min(box1[3], box2[3])\n",
        "                    \n",
        "                    if x2 > x1 and y2 > y1:\n",
        "                        intersection = (x2 - x1) * (y2 - y1)\n",
        "                        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "                        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "                        union = area1 + area2 - intersection\n",
        "                        iou = intersection / union\n",
        "                        \n",
        "                        if iou > iou_threshold:\n",
        "                            keep = False\n",
        "                            break\n",
        "            \n",
        "            if keep:\n",
        "                keep_indices.append(i)\n",
        "        \n",
        "        detected_boxes = [detected_boxes[i] for i in keep_indices]\n",
        "        detected_scores = [detected_scores[i] for i in keep_indices]\n",
        "    \n",
        "    # Stampa risultati\n",
        "    print(f\"✅ Trovate {len(detected_boxes)} persona/e nell'immagine:\")\n",
        "    for i, (box, score) in enumerate(zip(detected_boxes, detected_scores)):\n",
        "        x1, y1, x2, y2 = box\n",
        "        print(f\"   Persona {i+1}: Bounding box ({x1}, {y1}, {x2}, {y2}) - Confidence: {score:.3f}\")\n",
        "    \n",
        "    return detected_boxes, detected_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def draw_bounding_boxes(image_path, detected_boxes, detected_scores, output_path=None):\n",
        "    \"\"\"\n",
        "    Disegna le bounding box sull'immagine e salvala\n",
        "    \"\"\"\n",
        "    # Carica l'immagine preprocessata\n",
        "    img = Image.open(image_path)\n",
        "    \n",
        "    # Crea un oggetto per disegnare\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Colori per le bounding box (rosso per le persone)\n",
        "    box_color = (255, 0, 0)  # Rosso\n",
        "    text_color = (255, 255, 255)  # Bianco\n",
        "    \n",
        "    # Disegna ogni bounding box\n",
        "    for i, (box, score) in enumerate(zip(detected_boxes, detected_scores)):\n",
        "        x1, y1, x2, y2 = box\n",
        "        \n",
        "        # Disegna il rettangolo\n",
        "        draw.rectangle([x1, y1, x2, y2], outline=box_color, width=3)\n",
        "        \n",
        "        # Aggiungi testo con confidence score\n",
        "        text = f\"Person {i+1}: {score:.3f}\"\n",
        "        \n",
        "        # Posizione del testo (sopra la bounding box)\n",
        "        text_x = x1\n",
        "        text_y = max(0, y1 - 20)  # 20 pixel sopra la box\n",
        "        \n",
        "        # Disegna sfondo per il testo\n",
        "        text_bbox = draw.textbbox((text_x, text_y), text)\n",
        "        draw.rectangle([text_bbox[0]-2, text_bbox[1]-2, text_bbox[2]+2, text_bbox[3]+2], \n",
        "                      fill=box_color)\n",
        "        \n",
        "        # Disegna il testo\n",
        "        draw.text((text_x, text_y), text, fill=text_color)\n",
        "    \n",
        "    # Salva l'immagine\n",
        "    if output_path is None:\n",
        "        output_path = image_path.replace('.jpg', '_with_boxes.jpg').replace('.png', '_with_boxes.png')\n",
        "    \n",
        "    img.save(output_path)\n",
        "    print(f\"Immagine con bounding box salvata come: {output_path}\")\n",
        "    \n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Riscriviamo a mano la cella di inferenza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "kfszRNKjgVpa",
        "outputId": "06b038c3-82b1-44d9-95a1-2ae932545102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dettagli input modello: [{'name': 'images', 'index': 0, 'shape': array([  1, 640, 640,   3], dtype=int32), 'shape_signature': array([  1, 640, 640,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Dettagli output modello: [{'name': 'Identity', 'index': 10702, 'shape': array([   1,   84, 8400], dtype=int32), 'shape_signature': array([   1,   84, 8400], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "formato dell'input: <class 'numpy.float32'>\n",
            "formato output: <class 'numpy.float32'>\n",
            "Original image size: 640x480\n",
            "Immagine preprocessata salvata come: Foto/example_preprocessed.jpg\n",
            "Valore massimo nell'output: 0.9958502650260925\n",
            "Valore minimo nell'output: 5.9003391061907e-10\n",
            "Valore massimo per classe 'person': 0.00017405439575668424\n",
            "✅ Trovate 3 persona/e nell'immagine:\n",
            "   Persona 1: Bounding box (0, 210, 17, 248) - Confidence: 0.000\n",
            "   Persona 2: Bounding box (14, 1, 627, 68) - Confidence: 0.000\n",
            "   Persona 3: Bounding box (236, 459, 639, 557) - Confidence: 0.000\n",
            "Immagine con bounding box salvata come: Foto/example_preprocessed_with_boxes.jpg\n"
          ]
        }
      ],
      "source": [
        "#Proviamo a fare inferenza col modello quantizzato usando TensorFlow Lite\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#parametri modificabili per esperimenti\n",
        "\n",
        "\n",
        "#nomeModello = \"Modelli/Utili/yolo11n_int8.tflite\"\n",
        "#nomeModello = \"Modelli/Utili/yolo11n_float32.tflite\"\n",
        "nomeModello = \"Modelli/Utili/yolo11l_float32.tflite\"\n",
        "#nomeModello = \"Modelli/Utili/yolo11n_float16.tflite\"\n",
        "#nomeModello = \"yolo11n_int8personal.tflite\"\n",
        "nomeImmagine = \"Foto/example\"\n",
        "confidence_threshold = 0.000000001 #confidence in base a cui detecti o meno qualcosa\n",
        "iou_confidence = 0.1 #Intersection over Union, più è basso e più le bounding box contenute da altre, non sono considerate\n",
        "\n",
        "# Carica il modello\n",
        "interpreter = tf.lite.Interpreter(model_path=nomeModello)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Ottieni info su input/output\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Dettagli input modello:\", input_details)\n",
        "print(\"Dettagli output modello:\",output_details)\n",
        "\n",
        "#stampa formati input/output\n",
        "print(\"formato dell'input:\", input_details[0]['dtype'])\n",
        "print(\"formato output:\", output_details[0]['dtype'])\n",
        "\n",
        "# --- Carica e preprocessa immagine ---\n",
        "nomeImmagineJpg= nomeImmagine + \".jpg\"\n",
        "input_data = preprocess_image(nomeImmagineJpg)\n",
        "\n",
        "# --- QUANTIZZAZIONE INPUT ---\n",
        "if input_details[0]['dtype'] == np.int8:\n",
        "    # Ottieni i parametri di quantizzazione dell'input\n",
        "    quantization_params = input_details[0]['quantization_parameters']\n",
        "    scale = quantization_params['scales'][0]\n",
        "    zero_point = quantization_params['zero_points'][0]\n",
        "    \n",
        "    print(f\"Quantizzazione input: scale={scale}, zero_point={zero_point}\")\n",
        "    \n",
        "    # Quantizza l'input da float32 a int8\n",
        "    input_data = np.round(input_data / scale + zero_point).astype(np.int8)\n",
        "    print(\"Input quantizzato da float32 a int8\")\n",
        "\n",
        "# --- Imposta tensore input ---\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# --- Esegui inferenza ---\n",
        "interpreter.invoke()\n",
        "\n",
        "# --- Ottieni output ---\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# --- DEQUANTIZZAZIONE AUTOMATICA ---\n",
        "if output_details[0]['dtype'] == np.int8:\n",
        "    # Ottieni i parametri di quantizzazione dell'output\n",
        "    quantization_params = output_details[0]['quantization_parameters']\n",
        "    scale = quantization_params['scales'][0]\n",
        "    zero_point = quantization_params['zero_points'][0]\n",
        "        \n",
        "    print(f\"Dequantizzazione output: scale={scale}, zero_point={zero_point}\")\n",
        "        \n",
        "    # Dequantizza l'output da int8 a float32\n",
        "    output_data = (output_data.astype(np.float32) - zero_point) * scale\n",
        "    print(\"Output dequantizzato da int8 a float32\")\n",
        "\n",
        "# Esperimenti sull'output a mano.\n",
        "predictions = output_data[0] #output completo del modello\n",
        "# Dopo aver ottenuto output_data\n",
        "max_score = np.max(output_data)\n",
        "min_score = np.min(output_data) \n",
        "print(f\"Valore massimo nell'output: {max_score}\")\n",
        "print(f\"Valore minimo nell'output: {min_score}\")\n",
        "\n",
        "\n",
        "# Verifica i valori per la classe 'person'\n",
        "predictions = output_data[0]\n",
        "person_scores = predictions[5, :]  # era indice 5\n",
        "max_person_score = np.max(person_scores)\n",
        "print(f\"Valore massimo per classe 'person': {max_person_score}\")\n",
        "\n",
        "result = detect_person(output_data, confidence_threshold, iou_confidence) #0.000001 per float16/32\n",
        "if result:\n",
        "    detected_boxes, detected_scores = result\n",
        "    nomeImmagine_preprocessed = nomeImmagine + \"_preprocessed.jpg\"\n",
        "    # Disegna le bounding box sull'immagine preprocessata\n",
        "    draw_bounding_boxes(nomeImmagine_preprocessed, detected_boxes, detected_scores)\n",
        "else:\n",
        "    print(\"Nessuna bounding box da disegnare\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ridimensionamento delle immagini del dataset di calibrazione da 640x480 a 640x640\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "input_folder = \"immagini_calibrazione640x480\"\n",
        "output_folder = \"immagini_ridimensionate640x640\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.lower().endswith(\".jpg\"):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(img_path)  # dovrebbe essere 640x480\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Impossibile leggere {filename}\")\n",
        "            continue\n",
        "        \n",
        "        h, w, _ = img.shape\n",
        "        # Crop centrale quadrato 480x480\n",
        "        side = min(h, w)  # 480\n",
        "        start_x = (w - side) // 2  # (640-480)/2=80\n",
        "        start_y = (h - side) // 2  # (480-480)/2=0\n",
        "\n",
        "        img_cropped = img[start_y:start_y+side, start_x:start_x+side]\n",
        "\n",
        "        # Resize a 640x640\n",
        "        img_resized = cv2.resize(img_cropped, (640, 640))\n",
        "\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "        cv2.imwrite(output_path, img_resized)\n",
        "\n",
        "        print(f\"Elaborata {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1754471551.205323 6422266 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1754471551.206203 6422266 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-08-06 11:12:31.209411: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: yolo11n_saved_model\n",
            "2025-08-06 11:12:31.215852: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-08-06 11:12:31.215875: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: yolo11n_saved_model\n",
            "2025-08-06 11:12:31.257055: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-08-06 11:12:31.360434: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: yolo11n_saved_model\n",
            "2025-08-06 11:12:31.410747: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 201555 microseconds.\n",
            "2025-08-06 11:12:31.874916: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:4061] Estimated count of arithmetic ops: 7.648 G  ops, equivalently 3.824 G  MACs\n",
            "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n",
            "2025-08-06 11:14:47.203876: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:4061] Estimated count of arithmetic ops: 7.648 G  ops, equivalently 3.824 G  MACs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Modello quantizzato salvato come yolo11n_int8personal.tflite\n"
          ]
        }
      ],
      "source": [
        "#Quantizzazione del modello YOLO11n in int8, con dataset di calibrazione\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# === Dataset di calibrazione ===\n",
        "def representative_dataset_gen():\n",
        "    folder = 'immagini_ridimensionate640x640'  # la cartella con le immagini 640x640\n",
        "    image_paths = [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    for path in image_paths:\n",
        "        img = cv2.imread(path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = img.astype(np.float32) / 255.0         # Normalizzazione [0, 1]\n",
        "        img = np.expand_dims(img, axis=0)            # (1, 640, 640, 3)\n",
        "        yield [img]\n",
        "\n",
        "# === Conversione del modello ===\n",
        "saved_model_dir = \"yolo11n_saved_model\"  # percorso del modello saved_model\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Imposto input e output in INT8 perché è quantizzazione full int8\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# === Salvataggio ===\n",
        "output_path = \"yolo11n_int8personal.tflite\"\n",
        "with open(output_path, \"wb\") as f:\n",
        "    f.write(tflite_quant_model)\n",
        "\n",
        "print(f\"✅ Modello quantizzato salvato come {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TROVA TUTTE LE OPERAZIONI DEL MODELLO\n",
        "import tensorflow as tf\n",
        "\n",
        "nomeModello = \"Modelli/yolo11n_full_integer_quant.tflite\"\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=nomeModello)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "ops = set()\n",
        "for d in interpreter._get_ops_details():\n",
        "    ops.add(d['op_name'])\n",
        "\n",
        "print(\"Operazioni usate nel modello:\")\n",
        "for op in sorted(ops):\n",
        "    print(f\"- {op}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ESP32CAM Python (notebooks)",
      "language": "python",
      "name": "notebooks_venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
