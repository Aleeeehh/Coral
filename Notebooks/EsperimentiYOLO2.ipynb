{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312270ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formato dell'input del modello: <class 'numpy.float32'>\n",
      "formato output del modello: <class 'numpy.float32'>\n",
      "Shape output: (1, 84, 8400)\n",
      "predictions shape: (8400, 84)\n",
      "prima riga: [5.40647328e-01 1.51085123e-01 8.51498842e-02 4.00711894e-02\n",
      " 2.79087430e-07 6.26703880e-08 3.29604177e-07 6.01762480e-08\n",
      " 1.32663345e-07 7.62738921e-08 7.28267437e-08 6.29471231e-08\n",
      " 1.61372540e-07 6.91916711e-08 1.55344235e-08 6.17176070e-08\n",
      " 2.56964405e-08 1.74080597e-07 2.06534068e-07 6.85886832e-08\n",
      " 5.10500868e-08 7.56553575e-08 5.57399922e-08 8.17181132e-08\n",
      " 1.03012141e-07 6.16727718e-08 7.68923556e-08 7.43975193e-08\n",
      " 1.57854444e-07 3.22302526e-07 1.44844464e-07 1.74556220e-07\n",
      " 7.69884778e-08 1.58081775e-07 2.65187907e-07 9.33271380e-08\n",
      " 7.09081931e-08 1.48563984e-07 2.10989526e-07 4.97726766e-08\n",
      " 6.40138040e-08 2.40848721e-07 8.98006363e-08 4.40786572e-08\n",
      " 3.20233156e-08 6.13186657e-08 2.64282818e-07 2.86992559e-07\n",
      " 2.62695181e-07 1.63428197e-07 2.01161612e-07 1.06888720e-07\n",
      " 4.81617128e-08 8.73457680e-08 1.11337435e-07 2.39794446e-07\n",
      " 7.81161944e-08 6.61470594e-08 8.17549051e-08 7.24121847e-08\n",
      " 1.51358989e-07 4.54889673e-08 5.69330005e-08 7.92622430e-08\n",
      " 2.11714948e-07 6.67559377e-08 8.53935518e-08 7.68318031e-08\n",
      " 4.56273064e-08 8.79063293e-08 6.90768189e-08 1.64994077e-07\n",
      " 6.12792590e-08 6.34309032e-08 3.76398184e-08 1.75574144e-07\n",
      " 5.98577827e-08 2.10300286e-07 1.16186612e-07 2.98171479e-08\n",
      " 2.19675570e-07 5.79960222e-08 7.68867849e-08 1.61344218e-07]\n"
     ]
    }
   ],
   "source": [
    "# Esperimenti sull'output di inferenza a mano\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "nomeModello = \"Modelli/Utili/yolo11n_float32.tflite\"\n",
    "nomeImmagine = \"Foto/example4_preprocessed.jpg\"\n",
    "\n",
    "#converte l'immagine in RGB\n",
    "img = Image.open(nomeImmagine).convert('RGB')\n",
    "\n",
    "# Converti in un tensore 3d con valori float32 e normalizza (valori da 0 a 1)\n",
    "input_data =  np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "#aggiungi dimensione batch (1, 640, 640, 3) (Yolo si aspetta sempre un batch di input)\n",
    "#YOLO richiede input di forma (batch_size, height, width, channels) (ottenendo un tensore 4d)\n",
    "input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "#carica il modello\n",
    "interpreter = tf.lite.Interpreter(model_path=nomeModello)\n",
    "interpreter.allocate_tensors() #alloca memoria per i tensori\n",
    "\n",
    "#formati input/output modello\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"formato dell'input del modello:\", input_details[0]['dtype'])\n",
    "print(\"formato output del modello:\", output_details[0]['dtype'])\n",
    "\n",
    "# --- Imposta tensore input ---\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data) \n",
    "\n",
    "# --- Esegui inferenza ---\n",
    "interpreter.invoke()\n",
    "\n",
    "# --- Ottieni output ---\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predictions = output_data[0]\n",
    "\n",
    "# --- Analisi output ---\n",
    "print(\"Shape output:\", output_data.shape)\n",
    "predictions = predictions.T #faccio la trasposta per avere su ogni riga una predizione\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "#print(\"predictions:\", predictions)\n",
    "print(\"prima riga:\", predictions[1002])\n",
    "\n",
    "objectness = predictions[:, 4]\n",
    "class_0_probs = predictions[:, 5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47592567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP ðŸ’¡ Replace 'model=yolov5n.pt' with new 'model=yolov5nu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5nu.pt to 'yolov5nu.pt': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.31M/5.31M [00:00<00:00, 42.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Usiamo la libreria ultralytics per fare inferenza con YOLO11n\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\") #esporta in .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(format=\"onnx\") #esporta in .onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d81fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference and save results with Ultralytics\n",
    "nomeImmagine = \"../../Calibrazione/immagini_calibrazione640x480/capture(1).jpg\"\n",
    "results = model.predict(\n",
    "    nomeImmagine, \n",
    "    save=True, \n",
    "    project=\"../../FotoInference\",\n",
    "    name=\"detection\",\n",
    ")\n",
    "print(\"results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de977a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantizzazione del modello int int8 .espdl\n",
    "import sys, os\n",
    "import esp_ppq\n",
    "sys.path.append(os.path.abspath(\"esp-detection\"))\n",
    "from deploy.quantize import quant_espdet\n",
    "\n",
    "quant_espdet(\n",
    "    onnx_path=\"yolo11n.onnx\",      # onnx appena esportato\n",
    "    target=\"esp32s3\",              \n",
    "    num_of_bits=8,                 # quantizzazione int8\n",
    "    device='cpu',\n",
    "    batchsz=1,\n",
    "    imgsz=[640, 640],              # deve combaciare con la dimensione del modello\n",
    "    calib_dir=\"../../calibrazione_espressif/\",        #  cartella immagini reali\n",
    "    espdl_model_path=\"yolo11n.espdl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c38eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export ONNX di espressif\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.nn.modules import Detect, Attention\n",
    "from ultralytics.engine.exporter import Exporter, try_export, arange_patch\n",
    "from ultralytics.utils import LOGGER, __version__, colorstr\n",
    "from ultralytics.utils.checks import check_requirements\n",
    "from ultralytics.utils.torch_utils import get_latest_opset\n",
    "import torch\n",
    "import onnx\n",
    "\n",
    "\n",
    "class ESP_Detect(Detect):\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns predicted bounding boxes and class probabilities respectively.\"\"\"\n",
    "        # self.nl = 3\n",
    "        box0 = self.cv2[0](x[0])\n",
    "        score0 = self.cv3[0](x[0])\n",
    "\n",
    "        box1 = self.cv2[1](x[1])\n",
    "        score1 = self.cv3[1](x[1])\n",
    "\n",
    "        box2 = self.cv2[2](x[2])\n",
    "        score2 = self.cv3[2](x[2])\n",
    "\n",
    "        return box0, score0, box1, score1, box2, score2\n",
    "\n",
    "\n",
    "class ESP_Attention(Attention):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Attention module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): The output tensor after self-attention.\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        N = H * W\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.view(\n",
    "            -1, self.num_heads, self.key_dim * 2 + self.head_dim, N\n",
    "        ).split([self.key_dim, self.key_dim, self.head_dim], dim=2)\n",
    "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (v @ attn.transpose(-2, -1)).view(-1, C, H, W) + self.pe(\n",
    "            v.reshape(-1, C, H, W)\n",
    "        )\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ESP_Detect_Exporter(Exporter):\n",
    "    \"\"\"\n",
    "    adapted from ultralytics for detection task\n",
    "    \"\"\"\n",
    "\n",
    "    @try_export\n",
    "    def export_onnx(self, prefix=colorstr(\"ONNX:\")):\n",
    "        \"\"\"YOLO ONNX export.\"\"\"\n",
    "        requirements = [\"onnx>=1.14.0\"]  # from esp-ppq requirments.txt\n",
    "        # since onnxslim will cause NCHW -> 1(N*C)HW in yolo11, we replace onnxslim with onnxsim\n",
    "        if self.args.simplify:\n",
    "            requirements += [\n",
    "                \"onnxsim\",\n",
    "                \"onnxruntime\" + (\"-gpu\" if torch.cuda.is_available() else \"\"),\n",
    "            ]\n",
    "        check_requirements(requirements)\n",
    "\n",
    "        opset_version = self.args.opset or get_latest_opset()\n",
    "        LOGGER.info(\n",
    "            f\"\\n{prefix} starting export with onnx {onnx.__version__} opset {opset_version}...\"\n",
    "        )\n",
    "        f = str(self.file.with_suffix(\".onnx\"))\n",
    "        output_names = [\"box0\", \"score0\", \"box1\", \"score1\", \"box2\", \"score2\"]\n",
    "        dynamic = (\n",
    "            self.args.dynamic\n",
    "        )  # case 1: deploy model on ESP32, dynamic=False; case 2: QAT gt onnx for inference, dynamic=True\n",
    "        if dynamic:\n",
    "            dynamic = {\"images\": {0: \"batch\"}}\n",
    "            for name in output_names:\n",
    "                dynamic[name] = {0: \"batch\"}\n",
    "\n",
    "        with arange_patch(self.args):\n",
    "            torch.onnx.export(\n",
    "                self.model,\n",
    "                self.im,\n",
    "                f,\n",
    "                verbose=False,\n",
    "                opset_version=opset_version,\n",
    "                do_constant_folding=False,\n",
    "                input_names=[\"images\"],\n",
    "                output_names=output_names,\n",
    "                dynamic_axes=dynamic or None,\n",
    "            )\n",
    "        # Checks\n",
    "        model_onnx = onnx.load(f)  # load onnx model\n",
    "\n",
    "        # Simplify\n",
    "        if self.args.simplify:\n",
    "            try:\n",
    "                import onnxsim\n",
    "\n",
    "                LOGGER.info(\n",
    "                    f\"{prefix} simplifying with onnxsim {onnxsim.__version__}...\"\n",
    "                )\n",
    "                model_onnx, _ = onnxsim.simplify(model_onnx)\n",
    "\n",
    "            except Exception as e:\n",
    "                LOGGER.warning(f\"{prefix} simplifier failure: {e}\")\n",
    "\n",
    "        # Metadata\n",
    "        for k, v in self.metadata.items():\n",
    "            meta = model_onnx.metadata_props.add()\n",
    "            meta.key, meta.value = k, str(v)\n",
    "\n",
    "        onnx.save(model_onnx, f)\n",
    "        return f, model_onnx\n",
    "\n",
    "\n",
    "class ESP_YOLO(YOLO):\n",
    "    def export(\n",
    "        self,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._check_is_pytorch_model()\n",
    "        custom = {\n",
    "            \"imgsz\": self.model.args[\"imgsz\"],\n",
    "            \"batch\": 1,\n",
    "            \"data\": None,\n",
    "            \"device\": None,\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "        args = {**self.overrides, **custom, **kwargs, \"mode\": \"export\"}\n",
    "        return ESP_Detect_Exporter(overrides=args, _callbacks=self.callbacks)(\n",
    "            model=self.model\n",
    "        )\n",
    "\n",
    "\n",
    "model = ESP_YOLO(\"yolo11n.pt\")\n",
    "for m in model.modules():\n",
    "    if isinstance(m, Attention):\n",
    "        m.forward = ESP_Attention.forward.__get__(m)\n",
    "    if isinstance(m, Detect):\n",
    "        m.forward = ESP_Detect.forward.__get__(m)\n",
    "\n",
    "model.export(format=\"onnx\", simplify=True, opset=13, dynamic=False, imgsz=640)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewNotebooks2",
   "language": "python",
   "name": "esp32cam_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
