{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "inU1WWTTcHjg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python path: /Users/alessioprato/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/bin/python\n",
            "Python version: 3.10.18 (main, Jun  3 2025, 18:23:41) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 23.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import sys\n",
        "print(\"Python path:\", sys.executable)\n",
        "print(\"Python version:\", sys.version)\n",
        "\n",
        "#Carica il modello preaddestrato più piccolo\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "\n",
        "# Esporta in formato TFLite quantizzato \n",
        "#model.export(format='tflite', parametri vari..)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1j22semIVEhS"
      },
      "outputs": [],
      "source": [
        "# --- Funzione di preprocess immagine MODIFICATA per salvare l'immagine ---\n",
        "def preprocess_image(img_path, save_preprocessed=True):\n",
        "    # Carica immagine con PIL (RGB)\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # Immagine originale dimensioni 640x480\n",
        "    original_width, original_height = img.size\n",
        "    print(f\"Original image size: {original_width}x{original_height}\")\n",
        "\n",
        "    # Target size modello: 640x640\n",
        "    target_size = 640\n",
        "\n",
        "    # Crea canvas nero quadrato 640x640\n",
        "    new_img = Image.new('RGB', (target_size, target_size), (0, 0, 0))\n",
        "\n",
        "    # Calcola offset per centrare l'immagine originale sul canvas\n",
        "    offset_x = 0\n",
        "    offset_y = (target_size - original_height) // 2  # vertical padding\n",
        "\n",
        "    # Incolla immagine originale al centro (horizontal no padding)\n",
        "    new_img.paste(img, (offset_x, offset_y))\n",
        "\n",
        "    # Salva l'immagine preprocessata se richiesto\n",
        "    if save_preprocessed:\n",
        "        preprocessed_path = img_path.replace('.jpg', '_preprocessed.jpg').replace('.png', '_preprocessed.png')\n",
        "        new_img.save(preprocessed_path)\n",
        "        print(f\"Immagine preprocessata salvata come: {preprocessed_path}\")\n",
        "\n",
        "    # Converti in numpy array float32 e normalizza (se modello vuole 0-1)\n",
        "    input_array = np.array(new_img).astype(np.float32) / 255.0\n",
        "\n",
        "    # Aggiungi dimensione batch (1, 640, 640, 3)\n",
        "    input_array = np.expand_dims(input_array, axis=0)\n",
        "\n",
        "    return input_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image_320(img_path, save_preprocessed=True):\n",
        "    # Carica immagine con PIL (RGB) - già 320x320\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    \n",
        "    print(f\"Immagine originale: {img.size}\")\n",
        "    \n",
        "    # Salva se richiesto (opzionale, per debug)\n",
        "    if save_preprocessed:\n",
        "        preprocessed_path = img_path.replace('.jpg', '_preprocessed_320.jpg')\n",
        "        img.save(preprocessed_path)\n",
        "        print(f\"Immagine preprocessata salvata come: {preprocessed_path}\")\n",
        "    \n",
        "    # Solo normalizzazione e formato - NESSUN RESIZE\n",
        "    input_array = np.array(img).astype(np.float32) / 255.0\n",
        "    input_array = np.expand_dims(input_array, axis=0)  # (1, 320, 320, 3)\n",
        "    \n",
        "    return input_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Funzione di post-processing\n",
        "def detect_person(output_data, confidence_threshold=0.5, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Post-processing per rilevare persone nell'immagine\n",
        "    \"\"\"\n",
        "    # COCO class ID per 'person' è 0\n",
        "    PERSON_CLASS_ID = 0\n",
        "    \n",
        "    # Estrai le predizioni (rimuovi dimensione batch)\n",
        "    predictions = output_data[0]  # Shape: (84, 8400)\n",
        "    \n",
        "    # Separa coordinate, confidence e classi\n",
        "    # Ogni colonna: [x, y, w, h, confidence, class1, class2, ..., class80]\n",
        "    boxes = predictions[:4, :].T  # (8400, 4) - coordinate relative\n",
        "    confidences = predictions[4, :]  # (8400,) - confidence scores\n",
        "    class_scores = predictions[5:, :].T  # (8400, 79) - class scores\n",
        "    \n",
        "    # Trova le detections per la classe 'person'\n",
        "    person_scores = class_scores[:, PERSON_CLASS_ID]  # (8400,)\n",
        "    \n",
        "    # Combina confidence generale con confidence specifica della classe\n",
        "    final_scores = confidences * person_scores\n",
        "    \n",
        "    # Filtra detections con score alto\n",
        "    high_score_indices = np.where(final_scores > confidence_threshold)[0]\n",
        "    \n",
        "    if len(high_score_indices) == 0:\n",
        "        print(\"❌ Nessuna persona rilevata nell'immagine\")\n",
        "        return []\n",
        "    \n",
        "    # Estrai bounding boxes e scores\n",
        "    detected_boxes = []\n",
        "    detected_scores = []\n",
        "    \n",
        "    for idx in high_score_indices:\n",
        "        # Converti coordinate relative in pixel\n",
        "        x_center, y_center, width, height = boxes[idx]\n",
        "        \n",
        "        # Converti da formato YOLO (center, width, height) a formato pixel (x1, y1, x2, y2)\n",
        "        x1 = int((x_center - width/2) * 640)\n",
        "        y1 = int((y_center - height/2) * 640)\n",
        "        x2 = int((x_center + width/2) * 640)\n",
        "        y2 = int((y_center + height/2) * 640)\n",
        "        \n",
        "        # Clamp ai bordi dell'immagine\n",
        "        x1 = max(0, min(x1, 640))\n",
        "        y1 = max(0, min(y1, 640))\n",
        "        x2 = max(0, min(x2, 640))\n",
        "        y2 = max(0, min(y2, 640))\n",
        "        \n",
        "        detected_boxes.append([x1, y1, x2, y2])\n",
        "        detected_scores.append(final_scores[idx])\n",
        "    \n",
        "    # Non-maximum suppression per rimuovere duplicati\n",
        "    if len(detected_boxes) > 1:\n",
        "        # Implementazione semplificata di NMS\n",
        "        keep_indices = []\n",
        "        for i in range(len(detected_boxes)):\n",
        "            keep = True\n",
        "            for j in range(len(detected_boxes)):\n",
        "                if i != j and detected_scores[j] > detected_scores[i]:\n",
        "                    # Calcola IoU\n",
        "                    box1 = detected_boxes[i]\n",
        "                    box2 = detected_boxes[j]\n",
        "                    \n",
        "                    # Calcola area di intersezione\n",
        "                    x1 = max(box1[0], box2[0])\n",
        "                    y1 = max(box1[1], box2[1])\n",
        "                    x2 = min(box1[2], box2[2])\n",
        "                    y2 = min(box1[3], box2[3])\n",
        "                    \n",
        "                    if x2 > x1 and y2 > y1:\n",
        "                        intersection = (x2 - x1) * (y2 - y1)\n",
        "                        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "                        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "                        union = area1 + area2 - intersection\n",
        "                        iou = intersection / union\n",
        "                        \n",
        "                        if iou > iou_threshold:\n",
        "                            keep = False\n",
        "                            break\n",
        "            \n",
        "            if keep:\n",
        "                keep_indices.append(i)\n",
        "        \n",
        "        detected_boxes = [detected_boxes[i] for i in keep_indices]\n",
        "        detected_scores = [detected_scores[i] for i in keep_indices]\n",
        "    \n",
        "    # Stampa risultati\n",
        "    print(f\"✅ Trovate {len(detected_boxes)} persona/e nell'immagine:\")\n",
        "    for i, (box, score) in enumerate(zip(detected_boxes, detected_scores)):\n",
        "        x1, y1, x2, y2 = box\n",
        "        print(f\"   Persona {i+1}: Bounding box ({x1}, {y1}, {x2}, {y2}) - Confidence: {score:.3f}\")\n",
        "    \n",
        "    return detected_boxes, detected_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def draw_bounding_boxes(image_path, detected_boxes, detected_scores, output_path=None):\n",
        "    \"\"\"\n",
        "    Disegna le bounding box sull'immagine e salvala\n",
        "    \"\"\"\n",
        "    # Carica l'immagine preprocessata\n",
        "    img = Image.open(image_path)\n",
        "    \n",
        "    # Crea un oggetto per disegnare\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Colori per le bounding box (rosso per le persone)\n",
        "    box_color = (255, 0, 0)  # Rosso\n",
        "    text_color = (255, 255, 255)  # Bianco\n",
        "    \n",
        "    # Disegna ogni bounding box\n",
        "    for i, (box, score) in enumerate(zip(detected_boxes, detected_scores)):\n",
        "        x1, y1, x2, y2 = box\n",
        "        \n",
        "        # Disegna il rettangolo\n",
        "        draw.rectangle([x1, y1, x2, y2], outline=box_color, width=3)\n",
        "        \n",
        "        # Aggiungi testo con confidence score\n",
        "        text = f\"Person {i+1}: {score:.3f}\"\n",
        "        \n",
        "        # Posizione del testo (sopra la bounding box)\n",
        "        text_x = x1\n",
        "        text_y = max(0, y1 - 20)  # 20 pixel sopra la box\n",
        "        \n",
        "        # Disegna sfondo per il testo\n",
        "        text_bbox = draw.textbbox((text_x, text_y), text)\n",
        "        draw.rectangle([text_bbox[0]-2, text_bbox[1]-2, text_bbox[2]+2, text_bbox[3]+2], \n",
        "                      fill=box_color)\n",
        "        \n",
        "        # Disegna il testo\n",
        "        draw.text((text_x, text_y), text, fill=text_color)\n",
        "    \n",
        "    # Salva l'immagine\n",
        "    if output_path is None:\n",
        "        output_path = image_path.replace('.jpg', '_with_boxes.jpg').replace('.png', '_with_boxes.png')\n",
        "    \n",
        "    img.save(output_path)\n",
        "    print(f\"Immagine con bounding box salvata come: {output_path}\")\n",
        "    \n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "kfszRNKjgVpa",
        "outputId": "06b038c3-82b1-44d9-95a1-2ae932545102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dettagli input modello: [{'name': 'images', 'index': 0, 'shape': array([  1, 640, 640,   3], dtype=int32), 'shape_signature': array([  1, 640, 640,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Dettagli output modello: [{'name': 'Identity', 'index': 771, 'shape': array([   1,   84, 8400], dtype=int32), 'shape_signature': array([   1,   84, 8400], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "formato dell'input: <class 'numpy.float32'>\n",
            "formato output: <class 'numpy.float32'>\n",
            "\n",
            "Formato dei primi 5 tensori:\n",
            "Tensore 0: images - Dtype: <class 'numpy.float32'> - Shape: [  1 640 640   3]\n",
            "Tensore 1: arith.constant - Dtype: <class 'numpy.int32'> - Shape: [4 2]\n",
            "Tensore 2: arith.constant1 - Dtype: <class 'numpy.float16'> - Shape: [   1    2 8400]\n",
            "Original image size: 640x480\n",
            "Immagine preprocessata salvata come: Foto/example2_preprocessed.jpg\n",
            "Output inferenza: [[[  0.0060906    0.026556    0.037698 ...     0.79076     0.83317     0.88974]\n",
            "  [  0.0090692   0.0061092   0.0053846 ...     0.94134     0.94161     0.94369]\n",
            "  [   0.013994    0.051219    0.074031 ...     0.40976     0.32766      0.2217]\n",
            "  ...\n",
            "  [ 5.5984e-07  2.3214e-07  2.6116e-07 ...  3.1107e-06  3.1321e-06  3.8057e-06]\n",
            "  [ 2.2179e-07  1.5401e-07  1.8708e-07 ...  2.1496e-06  2.0725e-06  2.3471e-06]\n",
            "  [ 2.7891e-07   1.893e-07   2.425e-07 ...  1.8006e-06  1.7109e-06  2.0391e-06]]]\n",
            "Valore massimo nell'output: 0.9957919120788574\n",
            "Valore minimo nell'output: 5.095758884299251e-11\n",
            "Valore massimo per classe 'person': 0.0001915383036248386\n",
            "✅ Trovate 4 persona/e nell'immagine:\n",
            "   Persona 1: Bounding box (509, 424, 639, 506) - Confidence: 0.000\n",
            "   Persona 2: Bounding box (0, 170, 81, 422) - Confidence: 0.000\n",
            "   Persona 3: Bounding box (250, 215, 637, 507) - Confidence: 0.000\n",
            "   Persona 4: Bounding box (436, 371, 639, 506) - Confidence: 0.000\n",
            "Immagine con bounding box salvata come: Foto/example2_preprocessed_with_boxes.jpg\n"
          ]
        }
      ],
      "source": [
        "#Proviamo a fare inferenza col modello quantizzato usando TensorFlow Lite\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#parametri modificabili per esperimenti\n",
        "nomeModello = \"Modelli/Utili/yolo11n_float16.tflite\"\n",
        "nomeImmagine = \"Foto/example2\"\n",
        "confidence_threshold = 0.000001 #confidence in base a cui detecti o meno qualcosa\n",
        "iou_confidence = 0.5 #Intersection over Union, più è alto e più le bounding box contenute da altre, non sono considerate\n",
        "\n",
        "\n",
        "\n",
        "# Carica il modello quantizzato\n",
        "interpreter = tf.lite.Interpreter(model_path=nomeModello)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Ottieni info su input/output\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Dettagli input modello:\", input_details)\n",
        "print(\"Dettagli output modello:\",output_details)\n",
        "\n",
        "#stampa formati input/output\n",
        "print(\"formato dell'input:\", input_details[0]['dtype'])\n",
        "print(\"formato output:\", output_details[0]['dtype'])\n",
        "\n",
        "\n",
        "# Stampa i primi 5 tensori per vedere i formati dei pesi\n",
        "tensor_details = interpreter.get_tensor_details()\n",
        "print(\"\\nFormato dei primi 5 tensori:\")\n",
        "for i, tensor in enumerate(tensor_details[:3]):\n",
        "    print(f\"Tensore {i}: {tensor['name']} - Dtype: {tensor['dtype']} - Shape: {tensor['shape']}\")\n",
        "\n",
        "\n",
        "# --- Carica e preprocessa immagine ---\n",
        "nomeImmagineJpg= nomeImmagine + \".jpg\"\n",
        "input_data = preprocess_image(nomeImmagineJpg)\n",
        "\n",
        "\n",
        "# SE IL MODELLO SI ASPETTA INT8, CONVERTI DA FLOAT32! \n",
        "if input_details[0]['dtype'] == np.uint8:\n",
        "    # Converti da float32 (0-1) a uint8 (0-255)\n",
        "    input_data = (input_data * 255).astype(np.uint8)\n",
        "    print(\"Input convertito da float32 a uint8\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Imposta tensore input ---\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# --- Esegui inferenza ---\n",
        "interpreter.invoke()\n",
        "\n",
        "# --- Ottieni output ---\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "\n",
        "# --- DEQUANTIZZAZIONE AUTOMATICA ---\n",
        "if output_details[0]['dtype'] == np.uint8:\n",
        "    # Ottieni i parametri di quantizzazione\n",
        "    quantization_params = output_details[0]['quantization_parameters']\n",
        "    scale = quantization_params['scales'][0]\n",
        "    zero_point = quantization_params['zero_points'][0]\n",
        "        \n",
        "    print(f\"Dequantizzazione: scale={scale}, zero_point={zero_point}\")\n",
        "        \n",
        "    # Dequantizza l'output\n",
        "    output_data = (output_data.astype(np.float32) - zero_point) * scale\n",
        "    print(\"Output dequantizzato da uint8 a float32\")\n",
        "\n",
        "print(\"Output inferenza:\", output_data)\n",
        "\n",
        "# Dopo aver ottenuto output_data\n",
        "max_score = np.max(output_data)\n",
        "min_score = np.min(output_data)\n",
        "print(f\"Valore massimo nell'output: {max_score}\")\n",
        "print(f\"Valore minimo nell'output: {min_score}\")\n",
        "\n",
        "# Verifica i valori per la classe 'person'\n",
        "predictions = output_data[0]\n",
        "person_scores = predictions[5, :]  # Classe 'person' (indice 5)\n",
        "max_person_score = np.max(person_scores)\n",
        "print(f\"Valore massimo per classe 'person': {max_person_score}\")\n",
        "\n",
        "result = detect_person(output_data, confidence_threshold, iou_confidence) #0.000001 per float16/32\n",
        "if result:\n",
        "    detected_boxes, detected_scores = result\n",
        "    nomeImmagine_preprocessed = nomeImmagine + \"_preprocessed.jpg\"\n",
        "    # Disegna le bounding box sull'immagine preprocessata\n",
        "    draw_bounding_boxes(nomeImmagine_preprocessed, detected_boxes, detected_scores)\n",
        "else:\n",
        "    print(\"Nessuna bounding box da disegnare\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nuoge14geq0R",
        "outputId": "51339f98-8c35-494c-b0f9-aceebd9768a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1753367341.890280 3839891 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1753367341.890865 3839891 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Conversione completata: yolov11n_int8_true.tflite\n"
          ]
        }
      ],
      "source": [
        "#CELLA PER QUANTIZZAZIONE MANUALE INT8\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Carica il modello esportato da Ultralytics (in una cartella, non un file)\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"yolo11n_saved_model\")  # cambia path se necessario\n",
        "\n",
        "# Ottimizzazione full int8\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Dataset rappresentativo per la calibrazione (qui dummy, ma meglio usare immagini reali!)\n",
        "def representative_dataset():\n",
        "    for _ in range(100):\n",
        "        dummy_image = np.random.randint(0, 256, size=(1, 640, 640, 3), dtype=np.uint8)\n",
        "        yield [dummy_image.astype(np.float32)]  # o il tuo preprocessing\n",
        "\n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "# Forza input/output quantizzati\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "# Converte e salva il modello TFLite quantizzato\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "with open(\"yolov11n_int8_true.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_quant_model)\n",
        "\n",
        "print(\"✅ Conversione completata: yolov11n_int8_true.tflite\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ESP32CAM Python (notebooks)",
      "language": "python",
      "name": "notebooks_venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
