{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "inU1WWTTcHjg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python path: /Users/alessioprato/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/bin/python\n",
            "Python version: 3.10.18 (main, Jun  3 2025, 18:23:41) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 23.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import sys\n",
        "print(\"Python path:\", sys.executable)\n",
        "print(\"Python version:\", sys.version)\n",
        "\n",
        "#Carica il modello preaddestrato più piccolo\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "\n",
        "# Esporta in formato TFLite quantizzato \n",
        "#model.export(format='tflite', parametri vari..)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1j22semIVEhS"
      },
      "outputs": [],
      "source": [
        "# --- Funzione di preprocess immagine MODIFICATA per salvare l'immagine ---\n",
        "def preprocess_image(img_path, save_preprocessed=True):\n",
        "    # Carica immagine con PIL (RGB)\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # Immagine originale dimensioni 640x480\n",
        "    original_width, original_height = img.size\n",
        "    print(f\"Original image size: {original_width}x{original_height}\")\n",
        "\n",
        "    # Target size modello: 640x640\n",
        "    target_size = 640\n",
        "\n",
        "    # Crea canvas nero quadrato 640x640\n",
        "    new_img = Image.new('RGB', (target_size, target_size), (0, 0, 0))\n",
        "\n",
        "    # Calcola offset per centrare l'immagine originale sul canvas\n",
        "    offset_x = 0\n",
        "    offset_y = (target_size - original_height) // 2  # vertical padding\n",
        "\n",
        "    # Incolla immagine originale al centro (horizontal no padding)\n",
        "    new_img.paste(img, (offset_x, offset_y))\n",
        "\n",
        "    # Salva l'immagine preprocessata se richiesto\n",
        "    if save_preprocessed:\n",
        "        preprocessed_path = img_path.replace('.jpg', '_preprocessed.jpg').replace('.png', '_preprocessed.png')\n",
        "        new_img.save(preprocessed_path)\n",
        "        print(f\"Immagine preprocessata salvata come: {preprocessed_path}\")\n",
        "\n",
        "    # Converti in numpy array float32 e normalizza (se modello vuole 0-1)\n",
        "    input_array = np.array(new_img).astype(np.float32) / 255.0\n",
        "\n",
        "    # Aggiungi dimensione batch (1, 640, 640, 3)\n",
        "    input_array = np.expand_dims(input_array, axis=0)\n",
        "\n",
        "    return input_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image_320(img_path, save_preprocessed=True):\n",
        "    # Carica immagine con PIL (RGB) - già 320x320\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    \n",
        "    print(f\"Immagine originale: {img.size}\")\n",
        "    \n",
        "    # Salva se richiesto (opzionale, per debug)\n",
        "    if save_preprocessed:\n",
        "        preprocessed_path = img_path.replace('.jpg', '_preprocessed_320.jpg')\n",
        "        img.save(preprocessed_path)\n",
        "        print(f\"Immagine preprocessata salvata come: {preprocessed_path}\")\n",
        "    \n",
        "    # Solo normalizzazione e formato - NESSUN RESIZE\n",
        "    input_array = np.array(img).astype(np.float32) / 255.0\n",
        "    input_array = np.expand_dims(input_array, axis=0)  # (1, 320, 320, 3)\n",
        "    \n",
        "    return input_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Funzione di post-processing\n",
        "def detect_person(output_data, confidence_threshold=0.5, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Post-processing per rilevare persone nell'immagine\n",
        "    \"\"\"\n",
        "    # COCO class ID per 'person' è 0\n",
        "    PERSON_CLASS_ID = 0\n",
        "    \n",
        "    # Estrai le predizioni (rimuovi dimensione batch)\n",
        "    predictions = output_data[0]  # Shape: (84, 8400)\n",
        "    \n",
        "    # Separa coordinate, confidence e classi\n",
        "    # Ogni colonna: [x, y, w, h, confidence, class1, class2, ..., class80]\n",
        "    boxes = predictions[:4, :].T  # (8400, 4) - coordinate relative\n",
        "    confidences = predictions[4, :]  # (8400,) - confidence scores\n",
        "    class_scores = predictions[5:, :].T  # (8400, 79) - class scores\n",
        "    \n",
        "    # Trova le detections per la classe 'person'\n",
        "    person_scores = class_scores[:, PERSON_CLASS_ID]  # (8400,)\n",
        "    \n",
        "    # Combina confidence generale con confidence specifica della classe\n",
        "    final_scores = confidences * person_scores\n",
        "    \n",
        "    # Filtra detections con score alto\n",
        "    high_score_indices = np.where(final_scores > confidence_threshold)[0]\n",
        "    \n",
        "    if len(high_score_indices) == 0:\n",
        "        print(\"❌ Nessuna persona rilevata nell'immagine\")\n",
        "        return []\n",
        "    \n",
        "    # Estrai bounding boxes e scores\n",
        "    detected_boxes = []\n",
        "    detected_scores = []\n",
        "    \n",
        "    for idx in high_score_indices:\n",
        "        # Converti coordinate relative in pixel\n",
        "        x_center, y_center, width, height = boxes[idx]\n",
        "        \n",
        "        # Converti da formato YOLO (center, width, height) a formato pixel (x1, y1, x2, y2)\n",
        "        x1 = int((x_center - width/2) * 640)\n",
        "        y1 = int((y_center - height/2) * 640)\n",
        "        x2 = int((x_center + width/2) * 640)\n",
        "        y2 = int((y_center + height/2) * 640)\n",
        "        \n",
        "        # Clamp ai bordi dell'immagine\n",
        "        x1 = max(0, min(x1, 640))\n",
        "        y1 = max(0, min(y1, 640))\n",
        "        x2 = max(0, min(x2, 640))\n",
        "        y2 = max(0, min(y2, 640))\n",
        "        \n",
        "        detected_boxes.append([x1, y1, x2, y2])\n",
        "        detected_scores.append(final_scores[idx])\n",
        "    \n",
        "    # Non-maximum suppression per rimuovere duplicati\n",
        "    if len(detected_boxes) > 1:\n",
        "        # Implementazione semplificata di NMS\n",
        "        keep_indices = []\n",
        "        for i in range(len(detected_boxes)):\n",
        "            keep = True\n",
        "            for j in range(len(detected_boxes)):\n",
        "                if i != j and detected_scores[j] > detected_scores[i]:\n",
        "                    # Calcola IoU\n",
        "                    box1 = detected_boxes[i]\n",
        "                    box2 = detected_boxes[j]\n",
        "                    \n",
        "                    # Calcola area di intersezione\n",
        "                    x1 = max(box1[0], box2[0])\n",
        "                    y1 = max(box1[1], box2[1])\n",
        "                    x2 = min(box1[2], box2[2])\n",
        "                    y2 = min(box1[3], box2[3])\n",
        "                    \n",
        "                    if x2 > x1 and y2 > y1:\n",
        "                        intersection = (x2 - x1) * (y2 - y1)\n",
        "                        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "                        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "                        union = area1 + area2 - intersection\n",
        "                        iou = intersection / union\n",
        "                        \n",
        "                        if iou > iou_threshold:\n",
        "                            keep = False\n",
        "                            break\n",
        "            \n",
        "            if keep:\n",
        "                keep_indices.append(i)\n",
        "        \n",
        "        detected_boxes = [detected_boxes[i] for i in keep_indices]\n",
        "        detected_scores = [detected_scores[i] for i in keep_indices]\n",
        "    \n",
        "    # Stampa risultati\n",
        "    print(f\"✅ Trovate {len(detected_boxes)} persona/e nell'immagine:\")\n",
        "    for i, (box, score) in enumerate(zip(detected_boxes, detected_scores)):\n",
        "        x1, y1, x2, y2 = box\n",
        "        print(f\"   Persona {i+1}: Bounding box ({x1}, {y1}, {x2}, {y2}) - Confidence: {score:.3f}\")\n",
        "    \n",
        "    return detected_boxes, detected_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def draw_bounding_boxes(image_path, detected_boxes, detected_scores, output_path=None):\n",
        "    \"\"\"\n",
        "    Disegna le bounding box sull'immagine e salvala\n",
        "    \"\"\"\n",
        "    # Carica l'immagine preprocessata\n",
        "    img = Image.open(image_path)\n",
        "    \n",
        "    # Crea un oggetto per disegnare\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Colori per le bounding box (rosso per le persone)\n",
        "    box_color = (255, 0, 0)  # Rosso\n",
        "    text_color = (255, 255, 255)  # Bianco\n",
        "    \n",
        "    # Disegna ogni bounding box\n",
        "    for i, (box, score) in enumerate(zip(detected_boxes, detected_scores)):\n",
        "        x1, y1, x2, y2 = box\n",
        "        \n",
        "        # Disegna il rettangolo\n",
        "        draw.rectangle([x1, y1, x2, y2], outline=box_color, width=3)\n",
        "        \n",
        "        # Aggiungi testo con confidence score\n",
        "        text = f\"Person {i+1}: {score:.3f}\"\n",
        "        \n",
        "        # Posizione del testo (sopra la bounding box)\n",
        "        text_x = x1\n",
        "        text_y = max(0, y1 - 20)  # 20 pixel sopra la box\n",
        "        \n",
        "        # Disegna sfondo per il testo\n",
        "        text_bbox = draw.textbbox((text_x, text_y), text)\n",
        "        draw.rectangle([text_bbox[0]-2, text_bbox[1]-2, text_bbox[2]+2, text_bbox[3]+2], \n",
        "                      fill=box_color)\n",
        "        \n",
        "        # Disegna il testo\n",
        "        draw.text((text_x, text_y), text, fill=text_color)\n",
        "    \n",
        "    # Salva l'immagine\n",
        "    if output_path is None:\n",
        "        output_path = image_path.replace('.jpg', '_with_boxes.jpg').replace('.png', '_with_boxes.png')\n",
        "    \n",
        "    img.save(output_path)\n",
        "    print(f\"Immagine con bounding box salvata come: {output_path}\")\n",
        "    \n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "kfszRNKjgVpa",
        "outputId": "06b038c3-82b1-44d9-95a1-2ae932545102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dettagli input modello: [{'name': 'images', 'index': 0, 'shape': array([  1, 640, 640,   3], dtype=int32), 'shape_signature': array([  1, 640, 640,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Dettagli output modello: [{'name': 'Identity', 'index': 771, 'shape': array([   1,   84, 8400], dtype=int32), 'shape_signature': array([   1,   84, 8400], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "formato dell'input: <class 'numpy.float32'>\n",
            "formato output: <class 'numpy.float32'>\n",
            "\n",
            "Formato del tensore:\n",
            "Tensore: images - Dtype: <class 'numpy.float32'> - Shape: [  1 640 640   3]\n",
            "Original image size: 640x480\n",
            "Immagine preprocessata salvata come: Foto/lastExample_preprocessed.jpg\n",
            "Output inferenza: [[[6.2515810e-03 2.6880641e-02 3.7892532e-02 ... 8.0189681e-01\n",
            "   8.5269725e-01 8.9448059e-01]\n",
            "  [9.9672945e-03 6.1756298e-03 5.2996241e-03 ... 9.4070023e-01\n",
            "   9.4120586e-01 9.4356644e-01]\n",
            "  [1.4315648e-02 5.1900610e-02 7.4399807e-02 ... 3.8823098e-01\n",
            "   2.8881580e-01 2.1242070e-01]\n",
            "  ...\n",
            "  [5.9165160e-07 2.4813767e-07 2.8241976e-07 ... 2.2594493e-06\n",
            "   2.1413471e-06 2.8348343e-06]\n",
            "  [2.4568479e-07 1.7480276e-07 2.1675237e-07 ... 1.9346701e-06\n",
            "   1.9450486e-06 2.1211097e-06]\n",
            "  [3.0807601e-07 2.1178744e-07 2.8193563e-07 ... 1.6904663e-06\n",
            "   1.6786232e-06 1.9656279e-06]]]\n",
            "Valore massimo nell'output: 0.9947457909584045\n",
            "Valore minimo nell'output: 9.374459854048484e-20\n",
            "Valore massimo per classe 'person': 0.0001433247816748917\n",
            "✅ Trovate 2 persona/e nell'immagine:\n",
            "   Persona 1: Bounding box (0, 246, 170, 514) - Confidence: 0.000\n",
            "   Persona 2: Bounding box (128, 225, 546, 556) - Confidence: 0.000\n",
            "Immagine con bounding box salvata come: Foto/lastExample_preprocessed_with_boxes.jpg\n"
          ]
        }
      ],
      "source": [
        "#Proviamo a fare inferenza col modello quantizzato usando TensorFlow Lite\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#parametri modificabili per esperimenti\n",
        "nomeModello = \"Modelli/Utili/yolo11n_float16.tflite\"\n",
        "nomeImmagine = \"Foto/lastExample\"\n",
        "confidence_threshold = 0.0000001 #confidence in base a cui detecti o meno qualcosa\n",
        "iou_confidence = 0.5 #Intersection over Union, più è basso e più le bounding box contenute da altre, non sono considerate\n",
        "\n",
        "\n",
        "\n",
        "# Carica il modello quantizzato\n",
        "interpreter = tf.lite.Interpreter(model_path=nomeModello)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Ottieni info su input/output\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Dettagli input modello:\", input_details)\n",
        "print(\"Dettagli output modello:\",output_details)\n",
        "\n",
        "#stampa formati input/output\n",
        "print(\"formato dell'input:\", input_details[0]['dtype'])\n",
        "print(\"formato output:\", output_details[0]['dtype'])\n",
        "\n",
        "\n",
        "# Stampa un tensore per vedere i formati dei pesi\n",
        "tensor_details = interpreter.get_tensor_details()\n",
        "print(\"\\nFormato del tensore:\")\n",
        "print(f\"Tensore: {tensor_details[0]['name']} - Dtype: {tensor_details[0]['dtype']} - Shape: {tensor_details[0]['shape']}\")\n",
        "\n",
        "\n",
        "# --- Carica e preprocessa immagine ---\n",
        "nomeImmagineJpg= nomeImmagine + \".jpg\"\n",
        "input_data = preprocess_image(nomeImmagineJpg)\n",
        "\n",
        "\n",
        "# SE IL MODELLO SI ASPETTA INT8, CONVERTI DA FLOAT32! \n",
        "if input_details[0]['dtype'] == np.uint8:\n",
        "    # Converti da float32 (0-1) a uint8 (0-255)\n",
        "    input_data = (input_data * 255).astype(np.uint8)\n",
        "    print(\"Input convertito da float32 a uint8\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Imposta tensore input ---\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# --- Esegui inferenza ---\n",
        "interpreter.invoke()\n",
        "\n",
        "# --- Ottieni output ---\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "\n",
        "# --- DEQUANTIZZAZIONE AUTOMATICA ---\n",
        "if output_details[0]['dtype'] == np.uint8:\n",
        "    # Ottieni i parametri di quantizzazione\n",
        "    quantization_params = output_details[0]['quantization_parameters']\n",
        "    scale = quantization_params['scales'][0]\n",
        "    zero_point = quantization_params['zero_points'][0]\n",
        "        \n",
        "    print(f\"Dequantizzazione: scale={scale}, zero_point={zero_point}\")\n",
        "        \n",
        "    # Dequantizza l'output\n",
        "    output_data = (output_data.astype(np.float32) - zero_point) * scale\n",
        "    print(\"Output dequantizzato da uint8 a float32\")\n",
        "\n",
        "print(\"Output inferenza:\", output_data)\n",
        "\n",
        "# Dopo aver ottenuto output_data\n",
        "max_score = np.max(output_data)\n",
        "min_score = np.min(output_data)\n",
        "print(f\"Valore massimo nell'output: {max_score}\")\n",
        "print(f\"Valore minimo nell'output: {min_score}\")\n",
        "\n",
        "# Verifica i valori per la classe 'person'\n",
        "predictions = output_data[0]\n",
        "person_scores = predictions[5, :]  # Classe 'person' (indice 5)\n",
        "max_person_score = np.max(person_scores)\n",
        "print(f\"Valore massimo per classe 'person': {max_person_score}\")\n",
        "\n",
        "result = detect_person(output_data, confidence_threshold, iou_confidence) #0.000001 per float16/32\n",
        "if result:\n",
        "    detected_boxes, detected_scores = result\n",
        "    nomeImmagine_preprocessed = nomeImmagine + \"_preprocessed.jpg\"\n",
        "    # Disegna le bounding box sull'immagine preprocessata\n",
        "    draw_bounding_boxes(nomeImmagine_preprocessed, detected_boxes, detected_scores)\n",
        "else:\n",
        "    print(\"Nessuna bounding box da disegnare\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nuoge14geq0R",
        "outputId": "51339f98-8c35-494c-b0f9-aceebd9768a2"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "For full integer quantization, a `representative_dataset` must be specified.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m converter\u001b[38;5;241m.\u001b[39minference_input_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39muint8\n\u001b[1;32m      8\u001b[0m converter\u001b[38;5;241m.\u001b[39minference_output_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39muint8\n\u001b[0;32m----> 9\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_int8.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(tflite_model)\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1250\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1249\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1200\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wraps around convert function to export metrics.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;124;03m  The decorator to wrap the convert function.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_increase_conversion_attempt_metric()\n\u001b[0;32m-> 1200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_conversion_params_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m   1202\u001b[0m result \u001b[38;5;241m=\u001b[39m convert_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1022\u001b[0m, in \u001b[0;36mTFLiteConverterBase._save_conversion_params_metric\u001b[0;34m(self, graph_def, inference_type, inference_input_type)\u001b[0m\n\u001b[1;32m   1019\u001b[0m converter_kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_base_converter_args())\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# Optimization parameters.\u001b[39;00m\n\u001b[0;32m-> 1022\u001b[0m quant_mode \u001b[38;5;241m=\u001b[39m \u001b[43mQuantizationMode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentative_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_disable_per_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental_new_dynamic_range_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_low_bit_qat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_full_integer_quantization_bias_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_variable_quantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_strict_qdq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m converter_kwargs\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mtensorflowVersion,\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mapiVersion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivations_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: quant_mode\u001b[38;5;241m.\u001b[39mactivations_type(),\n\u001b[1;32m   1054\u001b[0m })\n\u001b[1;32m   1055\u001b[0m converter_kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   1056\u001b[0m     quant_mode\u001b[38;5;241m.\u001b[39mconverter_flags(inference_type, inference_input_type)\n\u001b[1;32m   1057\u001b[0m )\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:278\u001b[0m, in \u001b[0;36mQuantizationMode.__init__\u001b[0;34m(self, optimizations, target_spec, representative_dataset, graph_def, disable_per_channel, experimental_new_dynamic_range_quantizer, experimental_low_bit_qat, full_integer_quantization_bias_type, experimental_mlir_variable_quantization, experimental_qdq_annotation)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_def \u001b[38;5;241m=\u001b[39m graph_def\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_int8_target_required():\n\u001b[0;32m--> 278\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_int8_required\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_mlir_variable_quantization \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    281\u001b[0m     experimental_mlir_variable_quantization\n\u001b[1;32m    282\u001b[0m )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_float16_target_required():\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:510\u001b[0m, in \u001b[0;36mQuantizationMode._validate_int8_required\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# Check if representative_dataset is specified.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_representative_dataset\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_quantization_aware_training()\n\u001b[1;32m    509\u001b[0m ):\n\u001b[0;32m--> 510\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor full integer quantization, a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`representative_dataset` must be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m   )\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Update represenative dataset to the expected format.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_representative_dataset:\n",
            "\u001b[0;31mValueError\u001b[0m: For full integer quantization, a `representative_dataset` must be specified."
          ]
        }
      ],
      "source": [
        "#CALIBRAZIONE E QUANTIZZAZIONE INT8\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"yolo11n_saved_model\")\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model_int8.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Operazioni usate nel modello:\n",
            "- ADD\n",
            "- CONCATENATION\n",
            "- CONV_2D\n",
            "- DELEGATE\n",
            "- DEPTHWISE_CONV_2D\n",
            "- DEQUANTIZE\n",
            "- FULLY_CONNECTED\n",
            "- LOGISTIC\n",
            "- MAX_POOL_2D\n",
            "- MUL\n",
            "- PACK\n",
            "- PAD\n",
            "- RESHAPE\n",
            "- RESIZE_NEAREST_NEIGHBOR\n",
            "- SOFTMAX\n",
            "- SPLIT\n",
            "- STRIDED_SLICE\n",
            "- SUB\n",
            "- TRANSPOSE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alessioprato/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        }
      ],
      "source": [
        "#TROVA TUTTE LE OPERAZIONI DEL MODELLO\n",
        "import tensorflow as tf\n",
        "\n",
        "nomeModello = \"Modelli/Utili/yolo11n_float16.tflite\"\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=nomeModello)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "ops = set()\n",
        "for d in interpreter._get_ops_details():\n",
        "    ops.add(d['op_name'])\n",
        "\n",
        "print(\"Operazioni usate nel modello:\")\n",
        "for op in sorted(ops):\n",
        "    print(f\"- {op}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ESP32CAM Python (notebooks)",
      "language": "python",
      "name": "notebooks_venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
