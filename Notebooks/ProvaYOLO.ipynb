{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "inU1WWTTcHjg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python path: /Users/alessioprato/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/bin/python\n",
            "Python version: 3.10.18 (main, Jun  3 2025, 18:23:41) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 18.2MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.169 ğŸš€ Python-3.10.18 torch-2.7.1 CPU (Apple M1)\n",
            "WARNING âš ï¸ INT8 export requires a missing 'data' arg for calibration. Using default 'data=coco8.yaml'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['tf_keras', 'sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'ai-edge-litert>=1.2.0,<1.4.0', 'onnx>=1.12.0,<1.18.0', 'onnxslim>=0.1.59'] not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting tf_keras\n",
            "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting sng4onnx>=1.0.1\n",
            "  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting onnx_graphsurgeon>=0.3.26\n",
            "  Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting ai-edge-litert<1.4.0,>=1.2.0\n",
            "  Downloading ai_edge_litert-1.3.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (1.7 kB)\n",
            "Collecting onnx<1.18.0,>=1.12.0\n",
            "  Downloading onnx-1.17.0-cp310-cp310-macosx_12_0_universal2.whl.metadata (16 kB)\n",
            "Collecting onnxslim>=0.1.59\n",
            "  Downloading onnxslim-0.1.61-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting backports.strenum (from ai-edge-litert<1.4.0,>=1.2.0)\n",
            "  Downloading backports_strenum-1.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.10/site-packages (from ai-edge-litert<1.4.0,>=1.2.0) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.10/site-packages (from ai-edge-litert<1.4.0,>=1.2.0) (2.1.3)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from ai-edge-litert<1.4.0,>=1.2.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from ai-edge-litert<1.4.0,>=1.2.0) (4.14.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in ./.venv/lib/python3.10/site-packages (from onnx<1.18.0,>=1.12.0) (5.29.5)\n",
            "Requirement already satisfied: tensorflow<2.20,>=2.19 in ./.venv/lib/python3.10/site-packages (from tf_keras) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (25.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.32.4)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.73.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.37.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.1.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from onnxslim>=0.1.59) (1.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf_keras) (0.45.1)\n",
            "Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (14.0.0)\n",
            "Requirement already satisfied: namex in ./.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.0)\n",
            "Requirement already satisfied: optree in ./.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->onnxslim>=0.1.59) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.2)\n",
            "Downloading ai_edge_litert-1.3.0-cp310-cp310-macosx_12_0_arm64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-macosx_12_0_universal2.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n",
            "Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n",
            "Downloading onnxslim-0.1.61-py3-none-any.whl (157 kB)\n",
            "Downloading backports_strenum-1.3.1-py3-none-any.whl (8.3 kB)\n",
            "Installing collected packages: sng4onnx, onnx, backports.strenum, onnxslim, onnx_graphsurgeon, ai-edge-litert, tf_keras\n",
            "\u001b[2K  Attempting uninstall: onnx\n",
            "\u001b[2K    Found existing installation: onnx 1.18.0\n",
            "\u001b[2K    Uninstalling onnx-1.18.0:90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/7\u001b[0m [onnx]\n",
            "\u001b[2K      Successfully uninstalled onnx-1.18.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/7\u001b[0m [onnx]\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7/7\u001b[0m [tf_keras]6/7\u001b[0m [tf_keras]itert]\n",
            "\u001b[1A\u001b[2KSuccessfully installed ai-edge-litert-1.3.0 backports.strenum-1.3.1 onnx-1.17.0 onnx_graphsurgeon-0.5.8 onnxslim-0.1.61 sng4onnx-1.0.4 tf_keras-2.19.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 42.0s\n",
            "WARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.19.0...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/calibration_image_sample_data_20x128x128x3_float32.npy.zip to 'calibration_image_sample_data_20x128x128x3_float32.npy.zip'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.11M/1.11M [00:00<00:00, 7.34MB/s]\n",
            "Unzipping calibration_image_sample_data_20x128x128x3_float32.npy.zip to /Users/alessioprato/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/calibration_image_sample_data_20x128x128x3_float32.npy...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 50.75file/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.61...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 5.3s, saved as 'yolo11n.onnx' (10.3 MB)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m collecting INT8 calibration images from 'data=coco8.yaml'\n",
            "Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 158.9Â±91.7 MB/s, size: 69.2 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scanning /Users/alessioprato/Desktop/Tesi Nuova/Notebooks/datasets/coco8/labels/val... 2 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 666.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New cache created: /Users/alessioprato/Desktop/Tesi Nuova/Notebooks/datasets/coco8/labels/val.cache\n",
            "WARNING âš ï¸ \u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m >300 images recommended for INT8 calibration, found 2 images.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.28.2...\n",
            "Saved artifact at 'yolo11n_saved_model'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serving_default'\n",
            "  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 640, 640, 3), dtype=tf.float32, name='images')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(1, 84, 8400), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  13329226160: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  13329225808: TensorSpec(shape=(3, 3, 3, 16), dtype=tf.float32, name=None)\n",
            "  13329232320: TensorSpec(shape=(16,), dtype=tf.float32, name=None)\n",
            "  13329597360: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  13329608976: TensorSpec(shape=(3, 3, 16, 32), dtype=tf.float32, name=None)\n",
            "  13329719440: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13329722080: TensorSpec(shape=(1, 1, 32, 32), dtype=tf.float32, name=None)\n",
            "  13329883808: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13329854384: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13329852624: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13329855616: TensorSpec(shape=(3, 3, 16, 8), dtype=tf.float32, name=None)\n",
            "  13329857728: TensorSpec(shape=(8,), dtype=tf.float32, name=None)\n",
            "  13329725072: TensorSpec(shape=(3, 3, 8, 16), dtype=tf.float32, name=None)\n",
            "  13330067904: TensorSpec(shape=(16,), dtype=tf.float32, name=None)\n",
            "  13329857200: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13329857376: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13329725952: TensorSpec(shape=(1, 1, 48, 64), dtype=tf.float32, name=None)\n",
            "  13330063328: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13506886144: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  13506886496: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13506962608: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13506886672: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  13507044704: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13507152528: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13507232336: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13507043824: TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name=None)\n",
            "  13507045936: TensorSpec(shape=(16,), dtype=tf.float32, name=None)\n",
            "  13507155696: TensorSpec(shape=(3, 3, 16, 32), dtype=tf.float32, name=None)\n",
            "  13507242192: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13507153408: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13507165200: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13507467872: TensorSpec(shape=(1, 1, 96, 128), dtype=tf.float32, name=None)\n",
            "  13507466112: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13507475968: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  13507468752: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  13507537984: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13507292400: TensorSpec(shape=(1, 1, 128, 128), dtype=tf.float32, name=None)\n",
            "  13507532352: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13508806256: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13508805728: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13507687552: TensorSpec(shape=(1, 1, 64, 32), dtype=tf.float32, name=None)\n",
            "  13507688256: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13508898208: TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name=None)\n",
            "  13508900672: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13508900496: TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name=None)\n",
            "  13508815936: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13509091824: TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name=None)\n",
            "  13509198592: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13509208624: TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name=None)\n",
            "  13508809952: TensorSpec(shape=(1, 1, 64, 32), dtype=tf.float32, name=None)\n",
            "  13509201936: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13508808192: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13509322080: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  13509316976: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13507674704: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13507461712: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13509551984: TensorSpec(shape=(1, 1, 192, 128), dtype=tf.float32, name=None)\n",
            "  13509543712: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13509576480: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  13509557616: TensorSpec(shape=(3, 3, 128, 256), dtype=tf.float32, name=None)\n",
            "  13509588800: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13509550048: TensorSpec(shape=(1, 1, 256, 256), dtype=tf.float32, name=None)\n",
            "  13509685168: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13509829632: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13509831568: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13509677072: TensorSpec(shape=(1, 1, 128, 64), dtype=tf.float32, name=None)\n",
            "  13509834560: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13509914720: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13509988720: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13509986432: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13510075040: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13510183216: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13510195536: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13509994000: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13509910848: TensorSpec(shape=(1, 1, 128, 64), dtype=tf.float32, name=None)\n",
            "  13510321680: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13509911376: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13510325904: TensorSpec(shape=(1, 1, 128, 128), dtype=tf.float32, name=None)\n",
            "  13510448704: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13509833504: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13509835968: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13510580832: TensorSpec(shape=(1, 1, 384, 256), dtype=tf.float32, name=None)\n",
            "  13510576080: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13510365904: TensorSpec(shape=(1, 1, 256, 128), dtype=tf.float32, name=None)\n",
            "  13510363264: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13510792064: TensorSpec(shape=(1, 1, 512, 256), dtype=tf.float32, name=None)\n",
            "  13510791712: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13510798928: TensorSpec(shape=(1, 1, 256, 256), dtype=tf.float32, name=None)\n",
            "  13510802624: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13510851600: TensorSpec(shape=(1, 1, 128, 256), dtype=tf.float32, name=None)\n",
            "  13510940224: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13512048880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13511657600: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13511553648: TensorSpec(shape=(1, 1, 128, 128), dtype=tf.float32, name=None)\n",
            "  13512061728: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13512051344: TensorSpec(shape=(1, 1, 128, 256), dtype=tf.float32, name=None)\n",
            "  13512057680: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13512291280: TensorSpec(shape=(1, 1, 256, 128), dtype=tf.float32, name=None)\n",
            "  13512366512: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13512476448: TensorSpec(shape=(1, 1, 256, 256), dtype=tf.float32, name=None)\n",
            "  13512484192: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13512739648: TensorSpec(shape=(1, 1, 384, 128), dtype=tf.float32, name=None)\n",
            "  13512737712: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13512805712: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13512802544: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13512741584: TensorSpec(shape=(3, 3, 64, 32), dtype=tf.float32, name=None)\n",
            "  13512745632: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13512806416: TensorSpec(shape=(3, 3, 32, 64), dtype=tf.float32, name=None)\n",
            "  13512816096: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13512808176: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13512808704: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13512975712: TensorSpec(shape=(1, 1, 192, 128), dtype=tf.float32, name=None)\n",
            "  13513134624: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13513138496: TensorSpec(shape=(1, 1, 256, 64), dtype=tf.float32, name=None)\n",
            "  13512969904: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13513297408: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13513298992: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13513242784: TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name=None)\n",
            "  13513234688: TensorSpec(shape=(16,), dtype=tf.float32, name=None)\n",
            "  13513306208: TensorSpec(shape=(3, 3, 16, 32), dtype=tf.float32, name=None)\n",
            "  13513345152: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13513294416: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13513297936: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13513473040: TensorSpec(shape=(1, 1, 96, 64), dtype=tf.float32, name=None)\n",
            "  13513470224: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13513466704: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  13513472688: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13513618032: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13513972848: TensorSpec(shape=(1, 1, 192, 128), dtype=tf.float32, name=None)\n",
            "  13513973376: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13514363776: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13514362896: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13514285376: TensorSpec(shape=(3, 3, 64, 32), dtype=tf.float32, name=None)\n",
            "  13514374688: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  13514627152: TensorSpec(shape=(3, 3, 32, 64), dtype=tf.float32, name=None)\n",
            "  13514636832: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13514363072: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13514363248: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13516400848: TensorSpec(shape=(1, 1, 192, 128), dtype=tf.float32, name=None)\n",
            "  13516397328: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13515911088: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  13515911264: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  13515453744: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13512488416: TensorSpec(shape=(1, 1, 384, 256), dtype=tf.float32, name=None)\n",
            "  13511664464: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13328882272: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13327626688: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13317653248: TensorSpec(shape=(1, 1, 128, 64), dtype=tf.float32, name=None)\n",
            "  13021772816: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13516251984: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13516867712: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13517211776: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13517219520: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13514843488: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13514839440: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13515755888: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13517539632: TensorSpec(shape=(1, 1, 128, 64), dtype=tf.float32, name=None)\n",
            "  13514859696: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13517541392: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13517677216: TensorSpec(shape=(1, 1, 128, 128), dtype=tf.float32, name=None)\n",
            "  13517671584: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13328883152: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13328885440: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  13513643056: TensorSpec(shape=(3, 3, 64, 1), dtype=tf.float32, name=None)\n",
            "  13513642880: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13329089984: TensorSpec(shape=(1, 1, 384, 256), dtype=tf.float32, name=None)\n",
            "  13329124512: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13513858864: TensorSpec(shape=(1, 1, 64, 80), dtype=tf.float32, name=None)\n",
            "  13513854640: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13514510528: TensorSpec(shape=(3, 3, 256, 1), dtype=tf.float32, name=None)\n",
            "  13515447408: TensorSpec(shape=(3, 3, 128, 1), dtype=tf.float32, name=None)\n",
            "  13514510352: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  13515446528: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  13514997824: TensorSpec(shape=(1, 1, 256, 80), dtype=tf.float32, name=None)\n",
            "  13514634544: TensorSpec(shape=(1, 1, 128, 80), dtype=tf.float32, name=None)\n",
            "  13514057936: TensorSpec(shape=(3, 3, 80, 1), dtype=tf.float32, name=None)\n",
            "  13515011728: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13513979888: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13514052480: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13516260096: TensorSpec(shape=(3, 3, 80, 1), dtype=tf.float32, name=None)\n",
            "  13517214592: TensorSpec(shape=(3, 3, 256, 64), dtype=tf.float32, name=None)\n",
            "  13509087424: TensorSpec(shape=(3, 3, 80, 1), dtype=tf.float32, name=None)\n",
            "  13515450224: TensorSpec(shape=(3, 3, 128, 64), dtype=tf.float32, name=None)\n",
            "  13513639888: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13516259920: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13329136656: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13509083728: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13515452864: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13513640416: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13514374512: TensorSpec(shape=(1, 1, 80, 80), dtype=tf.float32, name=None)\n",
            "  13514364128: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13516563632: TensorSpec(shape=(1, 1, 80, 80), dtype=tf.float32, name=None)\n",
            "  13514517920: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13516637808: TensorSpec(shape=(1, 1, 80, 80), dtype=tf.float32, name=None)\n",
            "  13514457152: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13513856224: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  13516569968: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13515000640: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13516640624: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13514146384: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13513854112: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13516275408: TensorSpec(shape=(1, 1, 80, 80), dtype=tf.float32, name=None)\n",
            "  13515050496: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  13514413104: TensorSpec(shape=(1, 1, 80, 80), dtype=tf.float32, name=None)\n",
            "  13328883856: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  13514470352: TensorSpec(shape=(1, 1, 80, 80), dtype=tf.float32, name=None)\n",
            "  13514284320: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  13516568912: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13516271008: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13328883328: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13317652544: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13514284848: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  13514412576: TensorSpec(shape=(80,), dtype=tf.float32, name=None)\n",
            "  13515838336: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  13515833408: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  13516561344: TensorSpec(shape=(1, 1, 16, 1), dtype=tf.float32, name=None)\n",
            "  13515357376: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  13515357904: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  13515356144: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  13515342064: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  13515495856: TensorSpec(shape=(1, 2, 8400), dtype=tf.float32, name=None)\n",
            "  13515845728: TensorSpec(shape=(1, 2, 8400), dtype=tf.float32, name=None)\n",
            "  13515839920: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  13515842736: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1753452784.708263 4464848 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1753452784.709442 4464848 single_machine.cc:374] Starting new session\n",
            "W0000 00:00:1753452785.386537 4464848 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1753452785.386551 4464848 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "I0000 00:00:1753452786.131898 4464848 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
            "I0000 00:00:1753452786.131990 4464848 single_machine.cc:374] Starting new session\n",
            "W0000 00:00:1753452786.879048 4464848 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1753452786.879069 4464848 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "I0000 00:00:1753452787.595295 4464848 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
            "I0000 00:00:1753452787.595378 4464848 single_machine.cc:374] Starting new session\n",
            "W0000 00:00:1753452788.179325 4464848 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1753452788.179344 4464848 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1753452789.737004 4464848 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1753452789.737019 4464848 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "I0000 00:00:1753452789.768196 4464848 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
            "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n",
            "W0000 00:00:1753452797.034182 4464848 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1753452797.034198 4464848 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n",
            "W0000 00:00:1753452804.747858 4464848 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1753452804.747879 4464848 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "W0000 00:00:1753452816.189286 4464848 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1753452816.189300 4464848 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 136.0s, saved as 'yolo11n_saved_model' (34.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.19.0...\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 0.0s, saved as 'yolo11n_saved_model/yolo11n_int8.tflite' (2.8 MB)\n",
            "\n",
            "Export complete (136.4s)\n",
            "Results saved to \u001b[1m/Users/alessioprato/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo11n_saved_model/yolo11n_int8.tflite imgsz=640 int8 \n",
            "Validate:        yolo val task=detect model=yolo11n_saved_model/yolo11n_int8.tflite imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml int8 \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'yolo11n_saved_model/yolo11n_int8.tflite'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import sys\n",
        "print(\"Python path:\", sys.executable)\n",
        "print(\"Python version:\", sys.version)\n",
        "\n",
        "#Carica il modello preaddestrato piÃ¹ piccolo\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "\n",
        "# Esporta in formato TFLite quantizzato \n",
        "model.export(format='tflite', int8=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1j22semIVEhS"
      },
      "outputs": [],
      "source": [
        "# --- Funzione di preprocess immagine MODIFICATA per salvare l'immagine ---\n",
        "def preprocess_image(img_path, save_preprocessed=True):\n",
        "    # Carica immagine con PIL (RGB)\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # Immagine originale dimensioni 640x480\n",
        "    original_width, original_height = img.size\n",
        "    print(f\"Original image size: {original_width}x{original_height}\")\n",
        "\n",
        "    # Target size modello: 640x640\n",
        "    target_size = 640\n",
        "\n",
        "    # Crea canvas nero quadrato 640x640\n",
        "    new_img = Image.new('RGB', (target_size, target_size), (0, 0, 0))\n",
        "\n",
        "    # Calcola offset per centrare l'immagine originale sul canvas\n",
        "    offset_x = 0\n",
        "    offset_y = (target_size - original_height) // 2  # vertical padding\n",
        "\n",
        "    # Incolla immagine originale al centro (horizontal no padding)\n",
        "    new_img.paste(img, (offset_x, offset_y))\n",
        "\n",
        "    # Salva l'immagine preprocessata se richiesto\n",
        "    if save_preprocessed:\n",
        "        preprocessed_path = img_path.replace('.jpg', '_preprocessed.jpg').replace('.png', '_preprocessed.png')\n",
        "        new_img.save(preprocessed_path)\n",
        "        print(f\"Immagine preprocessata salvata come: {preprocessed_path}\")\n",
        "\n",
        "    # Converti in numpy array float32 e normalizza (se modello vuole 0-1)\n",
        "    input_array = np.array(new_img).astype(np.float32) / 255.0\n",
        "\n",
        "    # Aggiungi dimensione batch (1, 640, 640, 3)\n",
        "    input_array = np.expand_dims(input_array, axis=0)\n",
        "\n",
        "    return input_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image_320(img_path, save_preprocessed=True):\n",
        "    # Carica immagine con PIL (RGB) - giÃ  320x320\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    \n",
        "    print(f\"Immagine originale: {img.size}\")\n",
        "    \n",
        "    # Salva se richiesto (opzionale, per debug)\n",
        "    if save_preprocessed:\n",
        "        preprocessed_path = img_path.replace('.jpg', '_preprocessed_320.jpg')\n",
        "        img.save(preprocessed_path)\n",
        "        print(f\"Immagine preprocessata salvata come: {preprocessed_path}\")\n",
        "    \n",
        "    # Solo normalizzazione e formato - NESSUN RESIZE\n",
        "    input_array = np.array(img).astype(np.float32) / 255.0\n",
        "    input_array = np.expand_dims(input_array, axis=0)  # (1, 320, 320, 3)\n",
        "    \n",
        "    return input_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Funzione di post-processing\n",
        "def detect_person(output_data, confidence_threshold=0.5, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Post-processing per rilevare persone nell'immagine\n",
        "    \"\"\"\n",
        "    # COCO class ID per 'person' Ã¨ 0\n",
        "    PERSON_CLASS_ID = 0\n",
        "    \n",
        "    # Estrai le predizioni (rimuovi dimensione batch)\n",
        "    predictions = output_data[0]  # Shape: (84, 8400)\n",
        "    \n",
        "    # Separa coordinate, confidence e classi\n",
        "    # Ogni colonna: [x, y, w, h, confidence, class1, class2, ..., class80]\n",
        "    boxes = predictions[:4, :].T  # (8400, 4) - coordinate relative\n",
        "    confidences = predictions[4, :]  # (8400,) - confidence scores\n",
        "    class_scores = predictions[5:, :].T  # (8400, 79) - class scores\n",
        "    \n",
        "    # Trova le detections per la classe 'person'\n",
        "    person_scores = class_scores[:, PERSON_CLASS_ID]  # (8400,)\n",
        "    \n",
        "    # Combina confidence generale con confidence specifica della classe\n",
        "    final_scores = confidences * person_scores\n",
        "    \n",
        "    # Filtra detections con score alto\n",
        "    high_score_indices = np.where(final_scores > confidence_threshold)[0]\n",
        "    \n",
        "    if len(high_score_indices) == 0:\n",
        "        print(\"âŒ Nessuna persona rilevata nell'immagine\")\n",
        "        return []\n",
        "    \n",
        "    # Estrai bounding boxes e scores\n",
        "    detected_boxes = []\n",
        "    detected_scores = []\n",
        "    \n",
        "    for idx in high_score_indices:\n",
        "        # Converti coordinate relative in pixel\n",
        "        x_center, y_center, width, height = boxes[idx]\n",
        "        \n",
        "        # Converti da formato YOLO (center, width, height) a formato pixel (x1, y1, x2, y2)\n",
        "        x1 = int((x_center - width/2) * 640)\n",
        "        y1 = int((y_center - height/2) * 640)\n",
        "        x2 = int((x_center + width/2) * 640)\n",
        "        y2 = int((y_center + height/2) * 640)\n",
        "        \n",
        "        # Clamp ai bordi dell'immagine\n",
        "        x1 = max(0, min(x1, 640))\n",
        "        y1 = max(0, min(y1, 640))\n",
        "        x2 = max(0, min(x2, 640))\n",
        "        y2 = max(0, min(y2, 640))\n",
        "        \n",
        "        detected_boxes.append([x1, y1, x2, y2])\n",
        "        detected_scores.append(final_scores[idx])\n",
        "    \n",
        "    # Non-maximum suppression per rimuovere duplicati\n",
        "    if len(detected_boxes) > 1:\n",
        "        # Implementazione semplificata di NMS\n",
        "        keep_indices = []\n",
        "        for i in range(len(detected_boxes)):\n",
        "            keep = True\n",
        "            for j in range(len(detected_boxes)):\n",
        "                if i != j and detected_scores[j] > detected_scores[i]:\n",
        "                    # Calcola IoU\n",
        "                    box1 = detected_boxes[i]\n",
        "                    box2 = detected_boxes[j]\n",
        "                    \n",
        "                    # Calcola area di intersezione\n",
        "                    x1 = max(box1[0], box2[0])\n",
        "                    y1 = max(box1[1], box2[1])\n",
        "                    x2 = min(box1[2], box2[2])\n",
        "                    y2 = min(box1[3], box2[3])\n",
        "                    \n",
        "                    if x2 > x1 and y2 > y1:\n",
        "                        intersection = (x2 - x1) * (y2 - y1)\n",
        "                        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "                        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "                        union = area1 + area2 - intersection\n",
        "                        iou = intersection / union\n",
        "                        \n",
        "                        if iou > iou_threshold:\n",
        "                            keep = False\n",
        "                            break\n",
        "            \n",
        "            if keep:\n",
        "                keep_indices.append(i)\n",
        "        \n",
        "        detected_boxes = [detected_boxes[i] for i in keep_indices]\n",
        "        detected_scores = [detected_scores[i] for i in keep_indices]\n",
        "    \n",
        "    # Stampa risultati\n",
        "    print(f\"âœ… Trovate {len(detected_boxes)} persona/e nell'immagine:\")\n",
        "    for i, (box, score) in enumerate(zip(detected_boxes, detected_scores)):\n",
        "        x1, y1, x2, y2 = box\n",
        "        print(f\"   Persona {i+1}: Bounding box ({x1}, {y1}, {x2}, {y2}) - Confidence: {score:.3f}\")\n",
        "    \n",
        "    return detected_boxes, detected_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def draw_bounding_boxes(image_path, detected_boxes, detected_scores, output_path=None):\n",
        "    \"\"\"\n",
        "    Disegna le bounding box sull'immagine e salvala\n",
        "    \"\"\"\n",
        "    # Carica l'immagine preprocessata\n",
        "    img = Image.open(image_path)\n",
        "    \n",
        "    # Crea un oggetto per disegnare\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Colori per le bounding box (rosso per le persone)\n",
        "    box_color = (255, 0, 0)  # Rosso\n",
        "    text_color = (255, 255, 255)  # Bianco\n",
        "    \n",
        "    # Disegna ogni bounding box\n",
        "    for i, (box, score) in enumerate(zip(detected_boxes, detected_scores)):\n",
        "        x1, y1, x2, y2 = box\n",
        "        \n",
        "        # Disegna il rettangolo\n",
        "        draw.rectangle([x1, y1, x2, y2], outline=box_color, width=3)\n",
        "        \n",
        "        # Aggiungi testo con confidence score\n",
        "        text = f\"Person {i+1}: {score:.3f}\"\n",
        "        \n",
        "        # Posizione del testo (sopra la bounding box)\n",
        "        text_x = x1\n",
        "        text_y = max(0, y1 - 20)  # 20 pixel sopra la box\n",
        "        \n",
        "        # Disegna sfondo per il testo\n",
        "        text_bbox = draw.textbbox((text_x, text_y), text)\n",
        "        draw.rectangle([text_bbox[0]-2, text_bbox[1]-2, text_bbox[2]+2, text_bbox[3]+2], \n",
        "                      fill=box_color)\n",
        "        \n",
        "        # Disegna il testo\n",
        "        draw.text((text_x, text_y), text, fill=text_color)\n",
        "    \n",
        "    # Salva l'immagine\n",
        "    if output_path is None:\n",
        "        output_path = image_path.replace('.jpg', '_with_boxes.jpg').replace('.png', '_with_boxes.png')\n",
        "    \n",
        "    img.save(output_path)\n",
        "    print(f\"Immagine con bounding box salvata come: {output_path}\")\n",
        "    \n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "kfszRNKjgVpa",
        "outputId": "06b038c3-82b1-44d9-95a1-2ae932545102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dettagli input modello: [{'name': 'serving_default_images:0', 'index': 0, 'shape': array([  1, 640, 640,   3], dtype=int32), 'shape_signature': array([  1, 640, 640,   3], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.003921568859368563, -128), 'quantization_parameters': {'scales': array([0.00392157], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Dettagli output modello: [{'name': 'StatefulPartitionedCall:0', 'index': 594, 'shape': array([   1,   84, 8400], dtype=int32), 'shape_signature': array([   1,   84, 8400], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.003987358883023262, -124), 'quantization_parameters': {'scales': array([0.00398736], dtype=float32), 'zero_points': array([-124], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "formato dell'input: <class 'numpy.int8'>\n",
            "formato output: <class 'numpy.int8'>\n",
            "\n",
            "Formato del tensore:\n",
            "Tensore: serving_default_images:0 - Dtype: <class 'numpy.int8'> - Shape: [  1 640 640   3]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'preprocess_image' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# --- Carica e preprocessa immagine ---\u001b[39;00m\n\u001b[1;32m     41\u001b[0m nomeImmagineJpg\u001b[38;5;241m=\u001b[39m nomeImmagine \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m(nomeImmagineJpg)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# SE IL MODELLO SI ASPETTA INT8, CONVERTI DA FLOAT32! \u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mint8:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Ottieni i parametri di quantizzazione dell'input\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_image' is not defined"
          ]
        }
      ],
      "source": [
        "#Proviamo a fare inferenza col modello quantizzato usando TensorFlow Lite\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#parametri modificabili per esperimenti\n",
        "\n",
        "nomeModello = \"Modelli/yolo11n_full_integer_quant.tflite\"\n",
        "#nomeModello = \"Modelli/yolo11n_integer_quant.tflite\"\n",
        "#nomeModello = \"Modelli/yolo11n_int8.tflite\"\n",
        "#nomeModello = \"Modelli/Utili/yolo11n_float16.tflite\"\n",
        "nomeImmagine = \"Foto/example4\"\n",
        "confidence_threshold = 0.000000000001 #confidence in base a cui detecti o meno qualcosa\n",
        "iou_confidence = 0.1 #Intersection over Union, piÃ¹ Ã¨ basso e piÃ¹ le bounding box contenute da altre, non sono considerate\n",
        "\n",
        "\n",
        "\n",
        "# Carica il modello quantizzato\n",
        "interpreter = tf.lite.Interpreter(model_path=nomeModello)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Ottieni info su input/output\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Dettagli input modello:\", input_details)\n",
        "print(\"Dettagli output modello:\",output_details)\n",
        "\n",
        "#stampa formati input/output\n",
        "print(\"formato dell'input:\", input_details[0]['dtype'])\n",
        "print(\"formato output:\", output_details[0]['dtype'])\n",
        "\n",
        "\n",
        "# Stampa un tensore per vedere i formati dei pesi\n",
        "tensor_details = interpreter.get_tensor_details()\n",
        "print(\"\\nFormato del tensore:\")\n",
        "print(f\"Tensore: {tensor_details[0]['name']} - Dtype: {tensor_details[0]['dtype']} - Shape: {tensor_details[0]['shape']}\")\n",
        "\n",
        "\n",
        "# --- Carica e preprocessa immagine ---\n",
        "nomeImmagineJpg= nomeImmagine + \".jpg\"\n",
        "input_data = preprocess_image(nomeImmagineJpg)\n",
        "\n",
        "\n",
        "# SE IL MODELLO SI ASPETTA INT8, CONVERTI DA FLOAT32! \n",
        "if input_details[0]['dtype'] == np.int8:\n",
        "    # Ottieni i parametri di quantizzazione dell'input\n",
        "    quantization_params = input_details[0]['quantization_parameters']\n",
        "    scale = quantization_params['scales'][0]\n",
        "    zero_point = quantization_params['zero_points'][0]\n",
        "    \n",
        "    print(f\"Quantizzazione input: scale={scale}, zero_point={zero_point}\")\n",
        "    \n",
        "    # Quantizza l'input da float32 a int8\n",
        "    input_data = np.round(input_data / scale + zero_point).astype(np.int8)\n",
        "    print(\"Input quantizzato da float32 a int8\")\n",
        "\n",
        "\n",
        "# --- Imposta tensore input ---\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# --- Esegui inferenza ---\n",
        "interpreter.invoke()\n",
        "\n",
        "# --- Ottieni output ---\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Output PRIMA della dequantizzazione:\")\n",
        "print(f\"Tipo: {output_data.dtype}\")\n",
        "print(f\"Valore massimo: {np.max(output_data)}\")\n",
        "print(f\"Valore minimo: {np.min(output_data)}\")\n",
        "\n",
        "# --- DEQUANTIZZAZIONE AUTOMATICA ---\n",
        "if output_details[0]['dtype'] == np.int8:\n",
        "    # Ottieni i parametri di quantizzazione dell'output\n",
        "    quantization_params = output_details[0]['quantization_parameters']\n",
        "    scale = quantization_params['scales'][0]\n",
        "    zero_point = quantization_params['zero_points'][0]\n",
        "        \n",
        "    print(f\"Dequantizzazione output: scale={scale}, zero_point={zero_point}\")\n",
        "        \n",
        "    # Dequantizza l'output da int8 a float32\n",
        "    output_data = (output_data.astype(np.float32) - zero_point) * scale\n",
        "    print(\"Output dequantizzato da int8 a float32\")\n",
        "\n",
        "print(\"Output DOPO la dequantizzazione:\")\n",
        "print(f\"Tipo: {output_data.dtype}\")\n",
        "print(f\"Valore massimo: {np.max(output_data)}\")\n",
        "print(f\"Valore minimo: {np.min(output_data)}\")\n",
        "\n",
        "\n",
        "# Controlla specificamente l'indice 5\n",
        "predictions = output_data[0]\n",
        "person_scores = predictions[5, :]\n",
        "print(f\"Classe 'person' (indice 5) - Valore massimo: {np.max(person_scores)}\")\n",
        "print(f\"Classe 'person' (indice 5) - Valore minimo: {np.min(person_scores)}\")\n",
        "\n",
        "#print(\"Output inferenza:\", output_data)\n",
        "\n",
        "# Dopo aver ottenuto output_data\n",
        "max_score = np.max(output_data)\n",
        "min_score = np.min(output_data)\n",
        "print(f\"Valore massimo nell'output: {max_score}\")\n",
        "print(f\"Valore minimo nell'output: {min_score}\")\n",
        "\n",
        "# Verifica i valori per la classe 'person'\n",
        "predictions = output_data[0]\n",
        "person_scores = predictions[5, :]  # Classe 'person' (indice 5)\n",
        "max_person_score = np.max(person_scores)\n",
        "print(f\"Valore massimo per classe 'person': {max_person_score}\")\n",
        "\n",
        "result = detect_person(output_data, confidence_threshold, iou_confidence) #0.000001 per float16/32\n",
        "if result:\n",
        "    detected_boxes, detected_scores = result\n",
        "    nomeImmagine_preprocessed = nomeImmagine + \"_preprocessed.jpg\"\n",
        "    # Disegna le bounding box sull'immagine preprocessata\n",
        "    draw_bounding_boxes(nomeImmagine_preprocessed, detected_boxes, detected_scores)\n",
        "else:\n",
        "    print(\"Nessuna bounding box da disegnare\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dettagli input modello: [{'name': 'serving_default_images:0', 'index': 0, 'shape': array([  1, 640, 640,   3], dtype=int32), 'shape_signature': array([  1, 640, 640,   3], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.003921568859368563, -128), 'quantization_parameters': {'scales': array([0.00392157], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Dettagli output modello: [{'name': 'StatefulPartitionedCall:0', 'index': 594, 'shape': array([   1,   84, 8400], dtype=int32), 'shape_signature': array([   1,   84, 8400], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.003987358883023262, -124), 'quantization_parameters': {'scales': array([0.00398736], dtype=float32), 'zero_points': array([-124], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "formato dell'input: <class 'numpy.int8'>\n",
            "formato output: <class 'numpy.int8'>\n",
            "\n",
            "Formato del tensore:\n",
            "Tensore: serving_default_images:0 - Dtype: <class 'numpy.int8'> - Shape: [  1 640 640   3]\n",
            "Original image size: 481x640\n",
            "Immagine preprocessata salvata come: Foto/example4_preprocessed.jpg\n",
            "Quantizzazione input: scale=0.003921568859368563, zero_point=-128\n",
            "Input quantizzato da float32 a int8\n",
            "Output PRIMA della dequantizzazione:\n",
            "Tipo: int8\n",
            "Valore massimo: 125\n",
            "Valore minimo: -124\n",
            "Dequantizzazione output: scale=0.003987358883023262, zero_point=-124\n",
            "Output dequantizzato da int8 a float32\n",
            "Output DOPO la dequantizzazione:\n",
            "Tipo: float64\n",
            "Valore massimo: 0.9928523618727922\n",
            "Valore minimo: 0.0\n",
            "Classe 'person' (indice 5) - Valore massimo: 0.0\n",
            "Classe 'person' (indice 5) - Valore minimo: 0.0\n",
            "Valore massimo nell'output: 0.9928523618727922\n",
            "Valore minimo nell'output: 0.0\n",
            "Valore massimo per classe 'person': 0.0\n",
            "âŒ Nessuna persona rilevata nell'immagine\n",
            "Nessuna bounding box da disegnare\n"
          ]
        }
      ],
      "source": [
        "#Proviamo a fare inferenza col modello quantizzato usando TensorFlow Lite\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#parametri modificabili per esperimenti\n",
        "\n",
        "nomeModello = \"Modelli/yolo11n_full_integer_quant.tflite\"\n",
        "#nomeModello = \"Modelli/yolo11n_integer_quant.tflite\"\n",
        "#nomeModello = \"Modelli/yolo11n_int8.tflite\"\n",
        "#nomeModello = \"Modelli/Utili/yolo11n_float16.tflite\"\n",
        "nomeImmagine = \"Foto/example4\"\n",
        "confidence_threshold = 0.000000000001 #confidence in base a cui detecti o meno qualcosa\n",
        "iou_confidence = 0.1 #Intersection over Union, piÃ¹ Ã¨ basso e piÃ¹ le bounding box contenute da altre, non sono considerate\n",
        "\n",
        "\n",
        "\n",
        "# Carica il modello quantizzato\n",
        "interpreter = tf.lite.Interpreter(model_path=nomeModello)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Ottieni info su input/output\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Dettagli input modello:\", input_details)\n",
        "print(\"Dettagli output modello:\",output_details)\n",
        "\n",
        "#stampa formati input/output\n",
        "print(\"formato dell'input:\", input_details[0]['dtype'])\n",
        "print(\"formato output:\", output_details[0]['dtype'])\n",
        "\n",
        "\n",
        "# Stampa un tensore per vedere i formati dei pesi\n",
        "tensor_details = interpreter.get_tensor_details()\n",
        "print(\"\\nFormato del tensore:\")\n",
        "print(f\"Tensore: {tensor_details[0]['name']} - Dtype: {tensor_details[0]['dtype']} - Shape: {tensor_details[0]['shape']}\")\n",
        "\n",
        "\n",
        "# --- Carica e preprocessa immagine ---\n",
        "nomeImmagineJpg= nomeImmagine + \".jpg\"\n",
        "input_data = preprocess_image(nomeImmagineJpg)\n",
        "\n",
        "\n",
        "# SE IL MODELLO SI ASPETTA INT8, CONVERTI DA FLOAT32! \n",
        "if input_details[0]['dtype'] == np.int8:\n",
        "    # Ottieni i parametri di quantizzazione dell'input\n",
        "    quantization_params = input_details[0]['quantization_parameters']\n",
        "    scale = quantization_params['scales'][0]\n",
        "    zero_point = quantization_params['zero_points'][0]\n",
        "    \n",
        "    print(f\"Quantizzazione input: scale={scale}, zero_point={zero_point}\")\n",
        "    \n",
        "    # Quantizza l'input da float32 a int8\n",
        "    input_data = np.round(input_data / scale + zero_point).astype(np.int8)\n",
        "    print(\"Input quantizzato da float32 a int8\")\n",
        "\n",
        "\n",
        "# --- Imposta tensore input ---\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# --- Esegui inferenza ---\n",
        "interpreter.invoke()\n",
        "\n",
        "# --- Ottieni output ---\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Output PRIMA della dequantizzazione:\")\n",
        "print(f\"Tipo: {output_data.dtype}\")\n",
        "print(f\"Valore massimo: {np.max(output_data)}\")\n",
        "print(f\"Valore minimo: {np.min(output_data)}\")\n",
        "\n",
        "# --- DEQUANTIZZAZIONE AUTOMATICA ---\n",
        "if output_details[0]['dtype'] == np.int8:\n",
        "    # Ottieni i parametri di quantizzazione dell'output\n",
        "    quantization_params = output_details[0]['quantization_parameters']\n",
        "    scale = quantization_params['scales'][0]\n",
        "    zero_point = quantization_params['zero_points'][0]\n",
        "        \n",
        "    print(f\"Dequantizzazione output: scale={scale}, zero_point={zero_point}\")\n",
        "        \n",
        "    # Dequantizza l'output da int8 a float32\n",
        "    output_data = (output_data.astype(np.float32) - zero_point) * scale\n",
        "    print(\"Output dequantizzato da int8 a float32\")\n",
        "\n",
        "print(\"Output DOPO la dequantizzazione:\")\n",
        "print(f\"Tipo: {output_data.dtype}\")\n",
        "print(f\"Valore massimo: {np.max(output_data)}\")\n",
        "print(f\"Valore minimo: {np.min(output_data)}\")\n",
        "\n",
        "\n",
        "# Controlla specificamente l'indice 5\n",
        "predictions = output_data[0]\n",
        "person_scores = predictions[5, :]\n",
        "print(f\"Classe 'person' (indice 5) - Valore massimo: {np.max(person_scores)}\")\n",
        "print(f\"Classe 'person' (indice 5) - Valore minimo: {np.min(person_scores)}\")\n",
        "\n",
        "#print(\"Output inferenza:\", output_data)\n",
        "\n",
        "# Dopo aver ottenuto output_data\n",
        "max_score = np.max(output_data)\n",
        "min_score = np.min(output_data)\n",
        "print(f\"Valore massimo nell'output: {max_score}\")\n",
        "print(f\"Valore minimo nell'output: {min_score}\")\n",
        "\n",
        "# Verifica i valori per la classe 'person'\n",
        "predictions = output_data[0]\n",
        "person_scores = predictions[5, :]  # Classe 'person' (indice 5)\n",
        "max_person_score = np.max(person_scores)\n",
        "print(f\"Valore massimo per classe 'person': {max_person_score}\")\n",
        "\n",
        "result = detect_person(output_data, confidence_threshold, iou_confidence) #0.000001 per float16/32\n",
        "if result:\n",
        "    detected_boxes, detected_scores = result\n",
        "    nomeImmagine_preprocessed = nomeImmagine + \"_preprocessed.jpg\"\n",
        "    # Disegna le bounding box sull'immagine preprocessata\n",
        "    draw_bounding_boxes(nomeImmagine_preprocessed, detected_boxes, detected_scores)\n",
        "else:\n",
        "    print(\"Nessuna bounding box da disegnare\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nuoge14geq0R",
        "outputId": "51339f98-8c35-494c-b0f9-aceebd9768a2"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "For full integer quantization, a `representative_dataset` must be specified.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m converter\u001b[38;5;241m.\u001b[39minference_input_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39muint8\n\u001b[1;32m      8\u001b[0m converter\u001b[38;5;241m.\u001b[39minference_output_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39muint8\n\u001b[0;32m----> 9\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_int8.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(tflite_model)\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1250\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1249\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1200\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wraps around convert function to export metrics.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;124;03m  The decorator to wrap the convert function.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_increase_conversion_attempt_metric()\n\u001b[0;32m-> 1200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_conversion_params_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m   1202\u001b[0m result \u001b[38;5;241m=\u001b[39m convert_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1022\u001b[0m, in \u001b[0;36mTFLiteConverterBase._save_conversion_params_metric\u001b[0;34m(self, graph_def, inference_type, inference_input_type)\u001b[0m\n\u001b[1;32m   1019\u001b[0m converter_kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_base_converter_args())\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# Optimization parameters.\u001b[39;00m\n\u001b[0;32m-> 1022\u001b[0m quant_mode \u001b[38;5;241m=\u001b[39m \u001b[43mQuantizationMode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentative_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_disable_per_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental_new_dynamic_range_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_low_bit_qat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_full_integer_quantization_bias_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_variable_quantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_strict_qdq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m converter_kwargs\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mtensorflowVersion,\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mapiVersion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivations_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: quant_mode\u001b[38;5;241m.\u001b[39mactivations_type(),\n\u001b[1;32m   1054\u001b[0m })\n\u001b[1;32m   1055\u001b[0m converter_kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   1056\u001b[0m     quant_mode\u001b[38;5;241m.\u001b[39mconverter_flags(inference_type, inference_input_type)\n\u001b[1;32m   1057\u001b[0m )\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:278\u001b[0m, in \u001b[0;36mQuantizationMode.__init__\u001b[0;34m(self, optimizations, target_spec, representative_dataset, graph_def, disable_per_channel, experimental_new_dynamic_range_quantizer, experimental_low_bit_qat, full_integer_quantization_bias_type, experimental_mlir_variable_quantization, experimental_qdq_annotation)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_def \u001b[38;5;241m=\u001b[39m graph_def\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_int8_target_required():\n\u001b[0;32m--> 278\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_int8_required\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_mlir_variable_quantization \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    281\u001b[0m     experimental_mlir_variable_quantization\n\u001b[1;32m    282\u001b[0m )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_float16_target_required():\n",
            "File \u001b[0;32m~/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:510\u001b[0m, in \u001b[0;36mQuantizationMode._validate_int8_required\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# Check if representative_dataset is specified.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_representative_dataset\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_quantization_aware_training()\n\u001b[1;32m    509\u001b[0m ):\n\u001b[0;32m--> 510\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor full integer quantization, a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`representative_dataset` must be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m   )\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Update represenative dataset to the expected format.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_representative_dataset:\n",
            "\u001b[0;31mValueError\u001b[0m: For full integer quantization, a `representative_dataset` must be specified."
          ]
        }
      ],
      "source": [
        "#CALIBRAZIONE E QUANTIZZAZIONE INT8\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"yolo11n_saved_model\")\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model_int8.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Operazioni usate nel modello:\n",
            "- ADD\n",
            "- CONCATENATION\n",
            "- CONV_2D\n",
            "- DELEGATE\n",
            "- DEPTHWISE_CONV_2D\n",
            "- FULLY_CONNECTED\n",
            "- LOGISTIC\n",
            "- MAX_POOL_2D\n",
            "- MUL\n",
            "- PACK\n",
            "- PAD\n",
            "- QUANTIZE\n",
            "- RESHAPE\n",
            "- RESIZE_NEAREST_NEIGHBOR\n",
            "- SOFTMAX\n",
            "- SPLIT\n",
            "- STRIDED_SLICE\n",
            "- SUB\n",
            "- TRANSPOSE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alessioprato/Desktop/Tesi Nuova/ESP32CAM_ESPIDF/Notebooks/.venv/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        }
      ],
      "source": [
        "#TROVA TUTTE LE OPERAZIONI DEL MODELLO\n",
        "import tensorflow as tf\n",
        "\n",
        "nomeModello = \"Modelli/yolo11n_full_integer_quant.tflite\"\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=nomeModello)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "ops = set()\n",
        "for d in interpreter._get_ops_details():\n",
        "    ops.add(d['op_name'])\n",
        "\n",
        "print(\"Operazioni usate nel modello:\")\n",
        "for op in sorted(ops):\n",
        "    print(f\"- {op}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ESP32CAM Python (notebooks)",
      "language": "python",
      "name": "notebooks_venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
